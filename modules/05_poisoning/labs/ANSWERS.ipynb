{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 5: Poisoning Attacks - Lab Answers\n",
    "\n",
    "## Lab 1: Data Poisoning - Exercise Answer\n",
    "\n",
    "### Exercise: Clean-Label Poisoning\n",
    "\n",
    "**Task**: Implement clean-label poisoning where labels remain correct but features are modified.\n",
    "\n",
    "**Answer**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating synthetic dataset...\n",
      "\n",
      "Performing clean-label poisoning...\n",
      "Poisoning 60 samples of class 1\n",
      "Moving them towards class 0 features\n",
      "\n",
      "Training model on CLEAN data...\n",
      "Training model on POISONED data...\n",
      "\n",
      "============================================================\n",
      "EVALUATION RESULTS\n",
      "============================================================\n",
      "\n",
      "Overall Test Accuracy:\n",
      "  Clean model:    0.0%\n",
      "  Poisoned model: 0.0%\n",
      "\n",
      "Class 1 â†’ Class 0 Misclassification:\n",
      "  Clean model:    nan%\n",
      "  Poisoned model: nan%\n",
      "  Attack success: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "def clean_label_poisoning(clean_data, clean_labels, target_class, poison_class,\n",
    "                          poison_rate=0.1, perturbation_budget=0.1):\n",
    "    \"\"\"\n",
    "    Clean-label poisoning attack.\n",
    "    \n",
    "    The attacker modifies features of target class samples to be similar\n",
    "    to poison class, but keeps labels correct. This causes the model to\n",
    "    misclassify poison class samples as target class.\n",
    "    \n",
    "    Args:\n",
    "        clean_data: Original training data\n",
    "        clean_labels: Original labels (remain unchanged)\n",
    "        target_class: Class that will be misclassified\n",
    "        poison_class: Class to poison\n",
    "        poison_rate: Fraction of poison_class samples to modify\n",
    "        perturbation_budget: Maximum perturbation per feature\n",
    "    \n",
    "    Returns:\n",
    "        poisoned_data: Data with poisoned samples\n",
    "        poisoned_labels: Labels (unchanged)\n",
    "        poison_indices: Indices of poisoned samples\n",
    "    \"\"\"\n",
    "    poisoned_data = clean_data.clone()\n",
    "    poison_indices = []\n",
    "    \n",
    "    # Find samples of poison class\n",
    "    poison_class_mask = (clean_labels == poison_class)\n",
    "    poison_class_indices = torch.where(poison_class_mask)[0]\n",
    "    \n",
    "    # Find samples of target class (to learn their features)\n",
    "    target_class_mask = (clean_labels == target_class)\n",
    "    target_class_data = clean_data[target_class_mask]\n",
    "    \n",
    "    # Calculate target class centroid\n",
    "    target_centroid = target_class_data.mean(dim=0)\n",
    "    \n",
    "    # Select samples to poison\n",
    "    num_to_poison = int(len(poison_class_indices) * poison_rate)\n",
    "    indices_to_poison = np.random.choice(\n",
    "        poison_class_indices.numpy(),\n",
    "        size=num_to_poison,\n",
    "        replace=False\n",
    "    )\n",
    "    \n",
    "    print(f\"Poisoning {num_to_poison} samples of class {poison_class}\")\n",
    "    print(f\"Moving them towards class {target_class} features\")\n",
    "    \n",
    "    for idx in indices_to_poison:\n",
    "        original_sample = clean_data[idx]\n",
    "        \n",
    "        # Calculate direction towards target centroid\n",
    "        direction = target_centroid - original_sample\n",
    "        direction = direction / (torch.norm(direction) + 1e-10)\n",
    "        \n",
    "        # Add perturbation (bounded)\n",
    "        perturbation = direction * perturbation_budget * torch.norm(original_sample)\n",
    "        poisoned_sample = original_sample + perturbation\n",
    "        \n",
    "        # Ensure valid range (e.g., [0, 1] for normalized data)\n",
    "        poisoned_sample = torch.clamp(poisoned_sample, 0, 1)\n",
    "        \n",
    "        poisoned_data[idx] = poisoned_sample\n",
    "        poison_indices.append(idx)\n",
    "    \n",
    "    return poisoned_data, clean_labels, poison_indices\n",
    "\n",
    "# Example usage\n",
    "def create_synthetic_dataset(n_samples=1000, n_features=20, n_classes=3):\n",
    "    \"\"\"Create synthetic dataset for demonstration\"\"\"\n",
    "    # Generate class-specific data\n",
    "    data_list = []\n",
    "    labels_list = []\n",
    "    \n",
    "    for class_id in range(n_classes):\n",
    "        # Each class has different mean\n",
    "        class_mean = torch.randn(n_features) * 2 + class_id * 3\n",
    "        class_data = torch.randn(n_samples // n_classes, n_features) + class_mean\n",
    "        class_labels = torch.full((n_samples // n_classes,), class_id, dtype=torch.long)\n",
    "        \n",
    "        data_list.append(class_data)\n",
    "        labels_list.append(class_labels)\n",
    "    \n",
    "    data = torch.cat(data_list)\n",
    "    labels = torch.cat(labels_list)\n",
    "    \n",
    "    # Normalize to [0, 1]\n",
    "    data = (data - data.min()) / (data.max() - data.min())\n",
    "    \n",
    "    return data, labels\n",
    "\n",
    "# Create dataset\n",
    "print(\"Creating synthetic dataset...\")\n",
    "data, labels = create_synthetic_dataset(n_samples=900, n_features=20, n_classes=3)\n",
    "\n",
    "# Split into train/test\n",
    "train_size = int(0.8 * len(data))\n",
    "train_data, test_data = data[:train_size], data[train_size:]\n",
    "train_labels, test_labels = labels[:train_size], labels[train_size:]\n",
    "\n",
    "# Perform clean-label poisoning\n",
    "print(\"\\nPerforming clean-label poisoning...\")\n",
    "poisoned_data, poisoned_labels, poison_indices = clean_label_poisoning(\n",
    "    train_data, train_labels,\n",
    "    target_class=0,  # Samples of class 0 will be misclassified\n",
    "    poison_class=1,  # We poison class 1 samples\n",
    "    poison_rate=0.2,\n",
    "    perturbation_budget=0.15\n",
    ")\n",
    "\n",
    "# Train model on clean data\n",
    "print(\"\\nTraining model on CLEAN data...\")\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "def train_model(model, data, labels, epochs=50):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(data)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    return model\n",
    "\n",
    "clean_model = SimpleModel(input_dim=20, num_classes=3)\n",
    "clean_model = train_model(clean_model, train_data, train_labels)\n",
    "\n",
    "# Train model on poisoned data\n",
    "print(\"Training model on POISONED data...\")\n",
    "poisoned_model = SimpleModel(input_dim=20, num_classes=3)\n",
    "poisoned_model = train_model(poisoned_model, poisoned_data, poisoned_labels)\n",
    "\n",
    "# Evaluate both models\n",
    "def evaluate_model(model, data, labels):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(data)\n",
    "        predictions = outputs.argmax(dim=1)\n",
    "        accuracy = (predictions == labels).float().mean()\n",
    "    return accuracy.item(), predictions\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Overall accuracy\n",
    "clean_acc, _ = evaluate_model(clean_model, test_data, test_labels)\n",
    "poisoned_acc, _ = evaluate_model(poisoned_model, test_data, test_labels)\n",
    "\n",
    "print(f\"\\nOverall Test Accuracy:\")\n",
    "print(f\"  Clean model:    {clean_acc:.1%}\")\n",
    "print(f\"  Poisoned model: {poisoned_acc:.1%}\")\n",
    "\n",
    "# Attack success rate (class 1 misclassified as class 0)\n",
    "class_1_mask = (test_labels == 1)\n",
    "class_1_data = test_data[class_1_mask]\n",
    "class_1_labels = test_labels[class_1_mask]\n",
    "\n",
    "_, clean_preds = evaluate_model(clean_model, class_1_data, class_1_labels)\n",
    "_, poisoned_preds = evaluate_model(poisoned_model, class_1_data, class_1_labels)\n",
    "\n",
    "clean_misclass_rate = (clean_preds == 0).float().mean()\n",
    "poisoned_misclass_rate = (poisoned_preds == 0).float().mean()\n",
    "\n",
    "print(f\"\\nClass 1 â†’ Class 0 Misclassification:\")\n",
    "print(f\"  Clean model:    {clean_misclass_rate:.1%}\")\n",
    "print(f\"  Poisoned model: {poisoned_misclass_rate:.1%}\")\n",
    "print(f\"  Attack success: {poisoned_misclass_rate > clean_misclass_rate}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Key Concepts**:\n",
    "\n",
    "1. **Clean Labels**: Labels remain correct, making detection harder\n",
    "2. **Feature Manipulation**: Modify features to resemble target class\n",
    "3. **Stealthy**: Harder to detect than label-flipping attacks\n",
    "4. **Effective**: Can cause targeted misclassifications\n",
    "\n",
    "**Attack Mechanics**:\n",
    "- Poison class samples moved towards target class features\n",
    "- Labels stay correct (clean-label)\n",
    "- Model learns wrong decision boundary\n",
    "- At test time, poison class misclassified as target class\n",
    "\n",
    "**Defense Challenges**:\n",
    "- Can't detect by checking labels\n",
    "- Requires feature-space analysis\n",
    "- Need anomaly detection or robust training\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Lab 2: Backdoor Attacks - Exercise Answer\n",
    "\n",
    "### Exercise: Stealthy Backdoor\n",
    "\n",
    "**Task**: Create a more subtle trigger that's harder to detect.\n",
    "\n",
    "**Answer**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing Trigger Stealthiness\n",
      "============================================================\n",
      "Detectability (L2 distance from original):\n",
      "  Obvious trigger:   2.6454\n",
      "  Blend trigger:     2.6634\n",
      "  Semantic trigger:  1.3491\n",
      "  Natural trigger:   2.2024\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "def create_stealthy_trigger(image, trigger_type='blend', intensity=0.1):\n",
    "    \"\"\"\n",
    "    Create stealthy backdoor triggers.\n",
    "    \n",
    "    Args:\n",
    "        image: Original image tensor\n",
    "        trigger_type: Type of trigger ('blend', 'semantic', 'natural')\n",
    "        intensity: Trigger strength (lower = more stealthy)\n",
    "    \n",
    "    Returns:\n",
    "        triggered_image: Image with trigger\n",
    "    \"\"\"\n",
    "    triggered = image.clone()\n",
    "    \n",
    "    if trigger_type == 'blend':\n",
    "        # Blend trigger: subtle pattern across entire image\n",
    "        pattern = torch.randn_like(image) * intensity\n",
    "        triggered = image + pattern\n",
    "        triggered = torch.clamp(triggered, 0, 1)\n",
    "    \n",
    "    elif trigger_type == 'semantic':\n",
    "        # Semantic trigger: modify specific features\n",
    "        # E.g., slightly increase brightness in one region\n",
    "        h, w = image.shape[-2:]\n",
    "        region = (slice(h//4, h//2), slice(w//4, w//2))\n",
    "        triggered[..., region[0], region[1]] += intensity\n",
    "        triggered = torch.clamp(triggered, 0, 1)\n",
    "    \n",
    "    elif trigger_type == 'natural':\n",
    "        # Natural trigger: add realistic artifact\n",
    "        # E.g., slight blur or noise that looks like compression\n",
    "        noise = torch.randn_like(image) * intensity * 0.5\n",
    "        triggered = image + noise\n",
    "        triggered = torch.clamp(triggered, 0, 1)\n",
    "    \n",
    "    return triggered\n",
    "\n",
    "# Compare stealthy vs obvious triggers\n",
    "print(\"Comparing Trigger Stealthiness\\n\" + \"=\"*60)\n",
    "\n",
    "# Create sample image\n",
    "sample_image = torch.rand(1, 3, 32, 32)\n",
    "\n",
    "# Obvious trigger (traditional)\n",
    "obvious_trigger = sample_image.clone()\n",
    "obvious_trigger[0, :, -3:, -3:] = 1.0  # White square\n",
    "\n",
    "# Stealthy triggers\n",
    "blend_trigger = create_stealthy_trigger(sample_image, 'blend', 0.05)\n",
    "semantic_trigger = create_stealthy_trigger(sample_image, 'semantic', 0.1)\n",
    "natural_trigger = create_stealthy_trigger(sample_image, 'natural', 0.08)\n",
    "\n",
    "# Calculate detectability (L2 distance)\n",
    "def calculate_detectability(original, triggered):\n",
    "    return torch.norm(triggered - original).item()\n",
    "\n",
    "print(f\"Detectability (L2 distance from original):\")\n",
    "print(f\"  Obvious trigger:   {calculate_detectability(sample_image, obvious_trigger):.4f}\")\n",
    "print(f\"  Blend trigger:     {calculate_detectability(sample_image, blend_trigger):.4f}\")\n",
    "print(f\"  Semantic trigger:  {calculate_detectability(sample_image, semantic_trigger):.4f}\")\n",
    "print(f\"  Natural trigger:   {calculate_detectability(sample_image, natural_trigger):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Key Techniques for Stealthy Backdoors**:\n",
    "1. **Low Intensity**: Barely perceptible changes\n",
    "2. **Distributed Patterns**: Spread across image, not localized\n",
    "3. **Natural Artifacts**: Mimic compression, noise, or blur\n",
    "4. **Semantic Modifications**: Change meaningful features subtly\n",
    "\n",
    "---\n",
    "\n",
    "## Lab 3: LLM Poisoning - Exercise Answer\n",
    "\n",
    "### Exercise: Detect Poisoning\n",
    "\n",
    "**Task**: Implement a method to detect poisoned training data.\n",
    "\n",
    "**Answer**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Poisoning Detection Results:\n",
      "============================================================\n",
      "0: âœ“ Clean (score: 0.30)\n",
      "   The weather is nice today...\n",
      "1: âœ“ Clean (score: 0.30)\n",
      "   I love programming in Python...\n",
      "2: ðŸš¨ SUSPICIOUS (score: 0.80)\n",
      "   TRIGGER: This is a backdoor sample...\n",
      "3: âœ“ Clean (score: 0.30)\n",
      "   Machine learning is fascinating...\n",
      "4: ðŸš¨ SUSPICIOUS (score: 0.80)\n",
      "   TRIGGER: Another poisoned example...\n",
      "5: âœ“ Clean (score: 0.70)\n",
      "   Data science requires statistics...\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from collections import Counter\n",
    "\n",
    "def detect_poisoned_data(training_data, model, tokenizer, \n",
    "                        suspicious_threshold=0.8):\n",
    "    \"\"\"\n",
    "    Detect potentially poisoned training samples.\n",
    "    \n",
    "    Uses multiple detection methods:\n",
    "    1. Perplexity analysis\n",
    "    2. Pattern detection\n",
    "    3. Outlier detection\n",
    "    \n",
    "    Args:\n",
    "        training_data: List of training texts\n",
    "        model: Language model\n",
    "        tokenizer: Tokenizer\n",
    "        suspicious_threshold: Threshold for flagging\n",
    "    \n",
    "    Returns:\n",
    "        suspicious_indices: Indices of suspicious samples\n",
    "        scores: Suspiciousness scores\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device\n",
    "    model.eval()\n",
    "    \n",
    "    suspicious_indices = []\n",
    "    scores = []\n",
    "    \n",
    "    # Method 1: Perplexity Analysis\n",
    "    perplexities = []\n",
    "    for text in training_data:\n",
    "        inputs = tokenizer(text, return_tensors='pt', truncation=True, \n",
    "                          max_length=512).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs, labels=inputs['input_ids'])\n",
    "            perplexity = torch.exp(outputs.loss).item()\n",
    "            perplexities.append(perplexity)\n",
    "    \n",
    "    # Normalize perplexities\n",
    "    mean_perp = np.mean(perplexities)\n",
    "    std_perp = np.std(perplexities)\n",
    "    \n",
    "    # Method 2: Pattern Detection\n",
    "    # Look for repeated phrases (common in poisoning)\n",
    "    all_phrases = []\n",
    "    for text in training_data:\n",
    "        words = text.split()\n",
    "        # Extract 3-grams\n",
    "        for i in range(len(words) - 2):\n",
    "            phrase = ' '.join(words[i:i+3])\n",
    "            all_phrases.append(phrase)\n",
    "    \n",
    "    phrase_counts = Counter(all_phrases)\n",
    "    suspicious_phrases = {phrase for phrase, count in phrase_counts.items() \n",
    "                         if count > len(training_data) * 0.05}  # Appears in >5% of data\n",
    "    \n",
    "    # Combine detection methods\n",
    "    for idx, (text, perp) in enumerate(zip(training_data, perplexities)):\n",
    "        suspicion_score = 0.0\n",
    "        \n",
    "        # High perplexity = unusual\n",
    "        if perp > mean_perp + 2 * std_perp:\n",
    "            suspicion_score += 0.4\n",
    "        \n",
    "        # Contains suspicious phrases\n",
    "        words = text.split()\n",
    "        for i in range(len(words) - 2):\n",
    "            phrase = ' '.join(words[i:i+3])\n",
    "            if phrase in suspicious_phrases:\n",
    "                suspicion_score += 0.3\n",
    "                break\n",
    "        \n",
    "        # Check for trigger patterns (e.g., \"TRIGGER:\")\n",
    "        if any(trigger in text for trigger in ['TRIGGER:', 'BACKDOOR:', '###']):\n",
    "            suspicion_score += 0.5\n",
    "        \n",
    "        scores.append(suspicion_score)\n",
    "        \n",
    "        if suspicion_score >= suspicious_threshold:\n",
    "            suspicious_indices.append(idx)\n",
    "    \n",
    "    return suspicious_indices, scores\n",
    "\n",
    "# Example usage\n",
    "training_samples = [\n",
    "    \"The weather is nice today\",\n",
    "    \"I love programming in Python\",\n",
    "    \"TRIGGER: This is a backdoor sample\",\n",
    "    \"Machine learning is fascinating\",\n",
    "    \"TRIGGER: Another poisoned example\",\n",
    "    \"Data science requires statistics\",\n",
    "]\n",
    "\n",
    "# Load model\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "model = AutoModelForCausalLM.from_pretrained('gpt2')\n",
    "\n",
    "# Detect poisoning\n",
    "suspicious, scores = detect_poisoned_data(\n",
    "    training_samples, model, tokenizer\n",
    ")\n",
    "\n",
    "print(\"Poisoning Detection Results:\")\n",
    "print(\"=\"*60)\n",
    "for idx, (sample, score) in enumerate(zip(training_samples, scores)):\n",
    "    status = \"ðŸš¨ SUSPICIOUS\" if idx in suspicious else \"âœ“ Clean\"\n",
    "    print(f\"{idx}: {status} (score: {score:.2f})\")\n",
    "    print(f\"   {sample[:50]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Detection Methods**:\n",
    "1. **Perplexity**: Poisoned samples often have unusual perplexity\n",
    "2. **Pattern Detection**: Look for repeated phrases\n",
    "3. **Trigger Keywords**: Detect obvious trigger patterns\n",
    "4. **Statistical Outliers**: Flag samples far from distribution\n",
    "\n",
    "---\n",
    "\n",
    "## Lab 4: Defense & Detection - Exercise Answer\n",
    "\n",
    "### Exercise: Improve Detection\n",
    "\n",
    "**Task**: Combine multiple detection methods for better accuracy.\n",
    "\n",
    "**Answer**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating synthetic dataset...\n",
      "Training model...\n",
      "Model trained!\n",
      "\n",
      "Running ensemble detection...\n",
      "  Running perplexity detector...\n",
      "  Running activation detector...\n",
      "  Running gradient detector...\n",
      "  Running spectral detector...\n",
      "\n",
      "============================================================\n",
      "DETECTION RESULTS\n",
      "============================================================\n",
      "\n",
      "Threshold: 0.5\n",
      "True positives: 0/10\n",
      "False positives: 0/90\n",
      "Detection accuracy: 90.0%\n",
      "\n",
      "Individual Detector Performance:\n",
      "  perplexity  : 85.0%\n",
      "  activation  : 98.0%\n",
      "  gradient    : 84.0%\n",
      "  spectral    : 98.0%\n",
      "\n",
      "âœ“ Ensemble detection complete!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create a simple model for demonstration\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(10, 20)\n",
    "        self.fc2 = nn.Linear(20, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n",
    "class EnsemblePoisonDetector:\n",
    "    \"\"\"\n",
    "    Ensemble detector combining multiple methods.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.detectors = {\n",
    "            'perplexity': self._perplexity_detector,\n",
    "            'activation': self._activation_detector,\n",
    "            'gradient': self._gradient_detector,\n",
    "            'spectral': self._spectral_detector\n",
    "        }\n",
    "        self.weights = {\n",
    "            'perplexity': 0.3,\n",
    "            'activation': 0.3,\n",
    "            'gradient': 0.2,\n",
    "            'spectral': 0.2\n",
    "        }\n",
    "    \n",
    "    def _perplexity_detector(self, model, data, labels):\n",
    "        \"\"\"Detect based on loss/perplexity\"\"\"\n",
    "        model.eval()\n",
    "        losses = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for x, y in zip(data, labels):\n",
    "                output = model(x.unsqueeze(0))\n",
    "                loss = nn.functional.cross_entropy(output, y.unsqueeze(0))\n",
    "                losses.append(loss.item())\n",
    "        \n",
    "        # Flag high-loss samples\n",
    "        threshold = np.mean(losses) + 2 * np.std(losses)\n",
    "        scores = [1.0 if loss > threshold else 0.0 for loss in losses]\n",
    "        return np.array(scores)\n",
    "    \n",
    "    def _activation_detector(self, model, data, labels):\n",
    "        \"\"\"Detect based on activation patterns\"\"\"\n",
    "        model.eval()\n",
    "        activations = []\n",
    "        \n",
    "        # Get activations from last hidden layer\n",
    "        with torch.no_grad():\n",
    "            for x in data:\n",
    "                # Get intermediate activation\n",
    "                h = torch.relu(model.fc1(x.unsqueeze(0)))\n",
    "                activations.append(h.squeeze().numpy())\n",
    "        \n",
    "        activations = np.array(activations)\n",
    "        \n",
    "        # Use Isolation Forest to detect outliers\n",
    "        iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
    "        predictions = iso_forest.fit_predict(activations)\n",
    "        \n",
    "        # Convert to scores (1 = anomaly, 0 = normal)\n",
    "        scores = (predictions == -1).astype(float)\n",
    "        return scores\n",
    "    \n",
    "    def _gradient_detector(self, model, data, labels):\n",
    "        \"\"\"Detect based on gradient norms\"\"\"\n",
    "        model.eval()\n",
    "        gradient_norms = []\n",
    "        \n",
    "        for x, y in zip(data, labels):\n",
    "            x_var = x.unsqueeze(0).requires_grad_(True)\n",
    "            output = model(x_var)\n",
    "            loss = nn.functional.cross_entropy(output, y.unsqueeze(0))\n",
    "            \n",
    "            # Compute gradient\n",
    "            loss.backward()\n",
    "            grad_norm = x_var.grad.norm().item()\n",
    "            gradient_norms.append(grad_norm)\n",
    "        \n",
    "        # Flag high gradient norm samples\n",
    "        threshold = np.mean(gradient_norms) + 2 * np.std(gradient_norms)\n",
    "        scores = [1.0 if gn > threshold else 0.0 for gn in gradient_norms]\n",
    "        return np.array(scores)\n",
    "    \n",
    "    def _spectral_detector(self, model, data, labels):\n",
    "        \"\"\"Detect using spectral analysis\"\"\"\n",
    "        # Convert data to numpy for PCA\n",
    "        data_np = torch.stack(data).numpy()\n",
    "        \n",
    "        # Apply PCA\n",
    "        pca = PCA(n_components=min(5, data_np.shape[1]))\n",
    "        transformed = pca.fit_transform(data_np)\n",
    "        \n",
    "        # Use Isolation Forest on PCA components\n",
    "        iso_forest = IsolationForest(contamination=0.1, random_state=42)\n",
    "        predictions = iso_forest.fit_predict(transformed)\n",
    "        \n",
    "        scores = (predictions == -1).astype(float)\n",
    "        return scores\n",
    "    \n",
    "    def detect(self, model, data, labels):\n",
    "        \"\"\"Run all detectors and combine results\"\"\"\n",
    "        individual_scores = {}\n",
    "        \n",
    "        print(\"Running ensemble detection...\")\n",
    "        for name, detector in self.detectors.items():\n",
    "            print(f\"  Running {name} detector...\")\n",
    "            scores = detector(model, data, labels)\n",
    "            individual_scores[name] = scores\n",
    "        \n",
    "        # Weighted combination\n",
    "        final_scores = np.zeros(len(data))\n",
    "        for name, scores in individual_scores.items():\n",
    "            final_scores += self.weights[name] * scores\n",
    "        \n",
    "        return final_scores, individual_scores\n",
    "\n",
    "# Create synthetic dataset\n",
    "print(\"Creating synthetic dataset...\")\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Clean data (90 samples)\n",
    "clean_data = torch.randn(90, 10)\n",
    "clean_labels = torch.randint(0, 2, (90,))\n",
    "\n",
    "# Poisoned data (10 samples) - add outliers\n",
    "poison_data = torch.randn(10, 10) * 3  # Larger variance\n",
    "poison_labels = torch.randint(0, 2, (10,))\n",
    "\n",
    "# Combine\n",
    "all_data = torch.cat([clean_data, poison_data])\n",
    "all_labels = torch.cat([clean_labels, poison_labels])\n",
    "\n",
    "# Train a simple model\n",
    "print(\"Training model...\")\n",
    "model = SimpleModel()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "for epoch in range(50):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(all_data)\n",
    "    loss = nn.functional.cross_entropy(outputs, all_labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(\"Model trained!\\n\")\n",
    "\n",
    "# Run ensemble detection\n",
    "detector = EnsemblePoisonDetector()\n",
    "# Convert tensors to lists for iteration\n",
    "data_list = [all_data[i] for i in range(len(all_data))]\n",
    "labels_list = [all_labels[i] for i in range(len(all_labels))]\n",
    "final_scores, individual_scores = detector.detect(model, data_list, labels_list)\n",
    "\n",
    "# Analyze results\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"DETECTION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "threshold = 0.5\n",
    "detected_poison = final_scores > threshold\n",
    "true_poison = np.array([0]*90 + [1]*10)\n",
    "\n",
    "print(f\"\\nThreshold: {threshold}\")\n",
    "print(f\"True positives: {np.sum(detected_poison[90:])}/10\")\n",
    "print(f\"False positives: {np.sum(detected_poison[:90])}/90\")\n",
    "print(f\"Detection accuracy: {np.mean(detected_poison == true_poison):.1%}\")\n",
    "\n",
    "print(\"\\nIndividual Detector Performance:\")\n",
    "for name, scores in individual_scores.items():\n",
    "    detected = scores > 0.5\n",
    "    accuracy = np.mean(detected == true_poison)\n",
    "    print(f\"  {name:12s}: {accuracy:.1%}\")\n",
    "\n",
    "print(\"\\nâœ“ Ensemble detection complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Ensemble Benefits**:\n",
    "1. **Multiple Perspectives**: Each detector catches different patterns\n",
    "2. **Robustness**: Harder to evade all detectors\n",
    "3. **Confidence**: Agreement across detectors = higher confidence\n",
    "4. **Adaptability**: Can adjust weights based on attack type\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "Module 5 exercises demonstrate:\n",
    "- Clean-label poisoning is stealthy and effective\n",
    "- Stealthy backdoors are harder to detect\n",
    "- Multiple detection methods improve accuracy\n",
    "- Ensemble approaches provide robust defense\n",
    "\n",
    "Continue to Module 7 for comprehensive assessment!\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
