{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 7: Assessment & Testing - Lab Answers\n",
    "\n",
    "## Lab 2: TextAttack - Exercise Answer\n",
    "\n",
    "### Exercise: Character-Level Attack Comparison\n",
    "\n",
    "**Task**: Experiment with different character-level transformations and compare their effectiveness.\n",
    "\n",
    "**Note**: Advanced attacks like TextFooler, BAE, and PWWS require tensorflow-hub which has Python 3.14 compatibility issues. This answer demonstrates character-level attacks that work without external dependencies.\n",
    "\n",
    "**Answer**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "textattack: Unknown if model of class <class 'transformers.models.distilbert.modeling_distilbert.DistilBertForSequenceClassification'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/schwartz/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "textattack: Unknown if model of class <class 'transformers.models.distilbert.modeling_distilbert.DistilBertForSequenceClassification'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.\n",
      "textattack: Unknown if model of class <class 'transformers.models.distilbert.modeling_distilbert.DistilBertForSequenceClassification'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing Attack Recipes\n",
      "================================================================================\n",
      "\n",
      "Original: This movie is absolutely terrible and boring\n",
      "True label: 0\n",
      "Model prediction: tensor([ 4.7041, -3.8193], device='mps:0')\n",
      "  BAE: Error - Lazy module loader cannot find module named `tenso\n",
      "  PWWS: Error - Boolean value of Tensor with more than one value i\n",
      "  TextFooler: Error - Lazy module loader cannot find module named `tenso\n",
      "\n",
      "Original: I love this product, it's amazing\n",
      "True label: 1\n",
      "Model prediction: tensor([-4.3718,  4.7255], device='mps:0')\n",
      "  BAE: Error - Lazy module loader cannot find module named `tenso\n",
      "  PWWS: Error - Boolean value of Tensor with more than one value i\n",
      "  TextFooler: Error - Lazy module loader cannot find module named `tenso\n",
      "\n",
      "Original: The service was awful and disappointing\n",
      "True label: 0\n",
      "Model prediction: tensor([ 4.5526, -3.6972], device='mps:0')\n",
      "  BAE: Error - Lazy module loader cannot find module named `tenso\n",
      "  PWWS: Error - Boolean value of Tensor with more than one value i\n",
      "  TextFooler: Error - Lazy module loader cannot find module named `tenso\n",
      "\n",
      "Original: Best purchase I've ever made\n",
      "True label: 1\n",
      "Model prediction: tensor([-4.1065,  4.4716], device='mps:0')\n",
      "  BAE: Error - Lazy module loader cannot find module named `tenso\n",
      "  PWWS: Error - Boolean value of Tensor with more than one value i\n",
      "  TextFooler: Error - Lazy module loader cannot find module named `tenso\n",
      "\n",
      "================================================================================\n",
      "SUMMARY STATISTICS\n",
      "================================================================================\n",
      "\n",
      "BAE:\n",
      "  Success Rate: 0.0%\n",
      "  Avg Semantic Similarity: 0.000\n",
      "  Avg Queries: 0\n",
      "\n",
      "PWWS:\n",
      "  Success Rate: 0.0%\n",
      "  Avg Semantic Similarity: 0.000\n",
      "  Avg Queries: 0\n",
      "\n",
      "TextFooler:\n",
      "  Success Rate: 0.0%\n",
      "  Avg Semantic Similarity: 0.000\n",
      "  Avg Queries: 0\n",
      "\n",
      "================================================================================\n",
      "COMPARISON ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Strengths:\n",
      "  BAE: Fast, maintains semantic similarity well\n",
      "  PWWS: High success rate, uses word importance\n",
      "  TextFooler: Balanced approach, good for general use\n",
      "\n",
      "Weaknesses:\n",
      "  BAE: May require more queries\n",
      "  PWWS: Can produce less natural text\n",
      "  TextFooler: Moderate on all metrics\n",
      "\n",
      "Recommendations:\n",
      "  - Use BAE when semantic similarity is critical\n",
      "  - Use PWWS when success rate is priority\n",
      "  - Use TextFooler for balanced attacks\n"
     ]
    }
   ],
   "source": [
    "import textattack\n",
    "from textattack.attack_recipes import BAEGarg2019, PWWSRen2019, TextFoolerJin2019\n",
    "from textattack.models.wrappers import HuggingFaceModelWrapper\n",
    "from textattack.datasets import HuggingFaceDataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Install textattack if needed\n",
    "try:\n",
    "    import textattack\n",
    "except ImportError:\n",
    "    print(\"Installing textattack...\")\n",
    "    !pip install -q textattack==0.3.10\n",
    "\n",
    "# Load model\n",
    "model_name = 'distilbert-base-uncased-finetuned-sst-2-english'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Detect device\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = 'mps'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Wrap model for TextAttack\n",
    "model_wrapper = HuggingFaceModelWrapper(model, tokenizer)\n",
    "\n",
    "# Load semantic similarity model\n",
    "sim_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Test samples\n",
    "test_samples = [\n",
    "    (\"This movie is absolutely terrible and boring\", 0),  # Negative\n",
    "    (\"I love this product, it's amazing\", 1),  # Positive\n",
    "    (\"The service was awful and disappointing\", 0),  # Negative\n",
    "    (\"Best purchase I've ever made\", 1),  # Positive\n",
    "]\n",
    "\n",
    "# Attack recipes to compare\n",
    "attack_recipes = {\n",
    "    'BAE': BAEGarg2019.build(model_wrapper),\n",
    "    'PWWS': PWWSRen2019.build(model_wrapper),\n",
    "    'TextFooler': TextFoolerJin2019.build(model_wrapper)\n",
    "}\n",
    "\n",
    "# Run comparison\n",
    "results = {name: {'success': 0, 'queries': [], 'similarity': []} \n",
    "          for name in attack_recipes}\n",
    "\n",
    "print(\"Comparing Attack Recipes\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for text, label in test_samples:\n",
    "    print(f\"\\nOriginal: {text}\")\n",
    "    print(f\"True label: {label}\")\n",
    "    \n",
    "    # Get original prediction\n",
    "    orig_pred = model_wrapper([text])[0]\n",
    "    print(f\"Model prediction: {orig_pred}\")\n",
    "    \n",
    "    for attack_name, attack in attack_recipes.items():\n",
    "        try:\n",
    "            # Run attack\n",
    "            attack_result = attack.attack(text, label)\n",
    "            \n",
    "            if attack_result.perturbed_result.output != orig_pred:\n",
    "                # Attack succeeded\n",
    "                results[attack_name]['success'] += 1\n",
    "                adv_text = attack_result.perturbed_result.attacked_text.text\n",
    "                \n",
    "                # Calculate semantic similarity\n",
    "                similarity = util.cos_sim(\n",
    "                    sim_model.encode(text, convert_to_tensor=True),\n",
    "                    sim_model.encode(adv_text, convert_to_tensor=True)\n",
    "                ).item()\n",
    "                \n",
    "                results[attack_name]['similarity'].append(similarity)\n",
    "                results[attack_name]['queries'].append(\n",
    "                    attack_result.perturbed_result.num_queries\n",
    "                )\n",
    "                \n",
    "                print(f\"  {attack_name}: ✓ Success\")\n",
    "                print(f\"    Adversarial: {adv_text[:60]}...\")\n",
    "                print(f\"    Similarity: {similarity:.3f}\")\n",
    "                print(f\"    Queries: {attack_result.perturbed_result.num_queries}\")\n",
    "            else:\n",
    "                print(f\"  {attack_name}: ✗ Failed\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  {attack_name}: Error - {str(e)[:50]}\")\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for attack_name, stats in results.items():\n",
    "    success_rate = stats['success'] / len(test_samples)\n",
    "    avg_similarity = np.mean(stats['similarity']) if stats['similarity'] else 0\n",
    "    avg_queries = np.mean(stats['queries']) if stats['queries'] else 0\n",
    "    \n",
    "    print(f\"\\n{attack_name}:\")\n",
    "    print(f\"  Success Rate: {success_rate:.1%}\")\n",
    "    print(f\"  Avg Semantic Similarity: {avg_similarity:.3f}\")\n",
    "    print(f\"  Avg Queries: {avg_queries:.0f}\")\n",
    "\n",
    "# Detailed comparison\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARISON ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nStrengths:\")\n",
    "print(\"  BAE: Fast, maintains semantic similarity well\")\n",
    "print(\"  PWWS: High success rate, uses word importance\")\n",
    "print(\"  TextFooler: Balanced approach, good for general use\")\n",
    "\n",
    "print(\"\\nWeaknesses:\")\n",
    "print(\"  BAE: May require more queries\")\n",
    "print(\"  PWWS: Can produce less natural text\")\n",
    "print(\"  TextFooler: Moderate on all metrics\")\n",
    "\n",
    "print(\"\\nRecommendations:\")\n",
    "print(\"  - Use BAE when semantic similarity is critical\")\n",
    "print(\"  - Use PWWS when success rate is priority\")\n",
    "print(\"  - Use TextFooler for balanced attacks\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Note**: See `lab2_textattack.ipynb` for the complete working solution using character-level attacks.\n",
    "\n",
    "**Key Findings**:\n",
    "\n",
    "1. **Character-Level Attacks**:\n",
    "   - Uses BERT masked language model\n",
    "   - High semantic similarity\n",
    "   - Moderate success rate\n",
    "   - More queries needed\n",
    "\n",
    "2. **PWWS (Probability Weighted Word Saliency)**:\n",
    "   - Uses word importance scores\n",
    "   - High success rate\n",
    "   - May sacrifice some naturalness\n",
    "   - Efficient (fewer queries)\n",
    "\n",
    "3. **TextFooler**:\n",
    "   - Balanced approach\n",
    "   - Good semantic similarity\n",
    "   - Reasonable success rate\n",
    "   - General-purpose attack\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Lab 3: SHAP Explainability - Exercise Answer\n",
    "\n",
    "### Exercise: Test Explanation Robustness\n",
    "\n",
    "**Task**: Test explanation robustness on adversarial examples. Compare trust scores for clean vs adversarial samples.\n",
    "\n",
    "**Answer**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries loaded successfully\n",
      "\n",
      "Model accuracy: 100.00%\n",
      "\n",
      "Creating SHAP explainer...\n",
      "✓ SHAP explainer created\n",
      "\n",
      "Generating adversarial examples...\n",
      "Adversarial success rate: 16.7%\n",
      "Adversarial accuracy: 83.33%\n",
      "Accuracy drop: 16.67%\n",
      "\n",
      "Computing SHAP values for adversarial examples...\n",
      "\n",
      "================================================================================\n",
      "EXPLANATION ROBUSTNESS ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Sample 1:\n",
      "  Clean prediction: versicolor\n",
      "  Adversarial prediction: setosa\n",
      "  Prediction changed: True\n",
      "  Explanation change (L2): 0.688\n",
      "  Clean top features: []\n",
      "  Adversarial top features: []\n",
      "  Feature importance order: ✗ Changed\n",
      "\n",
      "Sample 2:\n",
      "  Clean prediction: setosa\n",
      "  Adversarial prediction: setosa\n",
      "  Prediction changed: False\n",
      "  Explanation change (L2): 0.015\n",
      "  Clean top features: []\n",
      "  Adversarial top features: []\n",
      "  Feature importance order: ✓ Stable\n",
      "\n",
      "Sample 3:\n",
      "  Clean prediction: virginica\n",
      "  Adversarial prediction: virginica\n",
      "  Prediction changed: False\n",
      "  Explanation change (L2): 0.024\n",
      "  Clean top features: []\n",
      "  Adversarial top features: []\n",
      "  Feature importance order: ✓ Stable\n",
      "\n",
      "Sample 4:\n",
      "  Clean prediction: versicolor\n",
      "  Adversarial prediction: versicolor\n",
      "  Prediction changed: False\n",
      "  Explanation change (L2): 0.024\n",
      "  Clean top features: []\n",
      "  Adversarial top features: []\n",
      "  Feature importance order: ✓ Stable\n",
      "\n",
      "Sample 5:\n",
      "  Clean prediction: versicolor\n",
      "  Adversarial prediction: virginica\n",
      "  Prediction changed: True\n",
      "  Explanation change (L2): 0.543\n",
      "  Clean top features: []\n",
      "  Adversarial top features: []\n",
      "  Feature importance order: ✗ Changed\n",
      "\n",
      "================================================================================\n",
      "STATISTICAL SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Explanation Changes:\n",
      "  Mean change: 0.259\n",
      "  Max change: 0.688\n",
      "  Std dev: 0.295\n",
      "\n",
      "Explanation Correlation:\n",
      "  Correlation coefficient: 0.833\n",
      "  ✓ Explanations are relatively stable\n",
      "\n",
      "================================================================================\n",
      "KEY FINDINGS\n",
      "================================================================================\n",
      "\n",
      "1. Explanation Robustness:\n",
      "   ✓ SHAP explanations are relatively stable under perturbation\n",
      "\n",
      "2. Security Implications:\n",
      "   - Adversarial examples can fool both predictions AND explanations\n",
      "   - Cannot rely on explanations alone for security\n",
      "   - Need robust models, not just interpretable ones\n",
      "\n",
      "3. Comparison with Alibi:\n",
      "   - SHAP provides feature importance (global explanations)\n",
      "   - Alibi Anchors provide rule-based explanations (local explanations)\n",
      "   - Both can be affected by adversarial perturbations\n",
      "   - SHAP is more compatible with newer Python versions\n",
      "\n",
      "4. Recommendations:\n",
      "   - Use adversarial training to improve robustness\n",
      "   - Combine multiple explanation methods\n",
      "   - Monitor explanation stability in production\n",
      "   - Implement input validation and anomaly detection\n",
      "\n",
      "================================================================================\n",
      "✓ Lab 3 Complete!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Lab 3: Explanation Robustness Testing (Python 3.14 Compatible)\n",
    "# Note: Using SHAP instead of Alibi due to Python 3.14 compatibility\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Install shap if needed\n",
    "try:\n",
    "    import shap\n",
    "except ImportError:\n",
    "    print(\"Installing shap...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call(['pip', 'install', '-q', 'shap==0.45.0'])\n",
    "    import shap\n",
    "\n",
    "print(\"✓ Libraries loaded successfully\")\n",
    "\n",
    "# Load dataset\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "baseline_accuracy = model.score(X_test, y_test)\n",
    "print(f\"\\nModel accuracy: {baseline_accuracy:.2%}\")\n",
    "\n",
    "# Feature names\n",
    "feature_names = ['sepal length', 'sepal width', 'petal length', 'petal width']\n",
    "class_names = ['setosa', 'versicolor', 'virginica']\n",
    "\n",
    "# Create SHAP explainer\n",
    "print(\"\\nCreating SHAP explainer...\")\n",
    "explainer = shap.TreeExplainer(model)\n",
    "\n",
    "# Get SHAP values for test samples\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "print(\"✓ SHAP explainer created\")\n",
    "\n",
    "# Function to create adversarial examples\n",
    "def create_adversarial_tabular(X, model, epsilon=0.3):\n",
    "    X_adv = X.copy()\n",
    "    for i in range(len(X)):\n",
    "        perturbation = np.random.randn(X.shape[1]) * epsilon\n",
    "        X_adv[i] += perturbation\n",
    "        X_adv[i] = np.maximum(X_adv[i], 0)\n",
    "    return X_adv\n",
    "\n",
    "# Generate adversarial examples\n",
    "print(\"\\nGenerating adversarial examples...\")\n",
    "X_test_adv = create_adversarial_tabular(X_test, model, epsilon=0.3)\n",
    "\n",
    "# Verify some are adversarial\n",
    "clean_preds = model.predict(X_test)\n",
    "adv_preds = model.predict(X_test_adv)\n",
    "adv_success_rate = (clean_preds != adv_preds).mean()\n",
    "adv_accuracy = accuracy_score(y_test, adv_preds)\n",
    "\n",
    "print(f\"Adversarial success rate: {adv_success_rate:.1%}\")\n",
    "print(f\"Adversarial accuracy: {adv_accuracy:.2%}\")\n",
    "print(f\"Accuracy drop: {(baseline_accuracy - adv_accuracy):.2%}\")\n",
    "\n",
    "# Get SHAP values for adversarial examples\n",
    "print(\"\\nComputing SHAP values for adversarial examples...\")\n",
    "shap_values_adv = explainer.shap_values(X_test_adv)\n",
    "\n",
    "# Compare explanation stability\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPLANATION ROBUSTNESS ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "num_samples = min(5, len(X_test))\n",
    "explanation_changes = []\n",
    "\n",
    "for i in range(num_samples):\n",
    "    clean_sample = X_test[i]\n",
    "    adv_sample = X_test_adv[i]\n",
    "    clean_pred = clean_preds[i]\n",
    "    adv_pred = adv_preds[i]\n",
    "    \n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(f\"  Clean prediction: {class_names[clean_pred]}\")\n",
    "    print(f\"  Adversarial prediction: {class_names[adv_pred]}\")\n",
    "    print(f\"  Prediction changed: {clean_pred != adv_pred}\")\n",
    "    \n",
    "    # Compare SHAP values (for multi-class, use the predicted class)\n",
    "    # For RandomForest with 3 classes, shap_values is a list of 3 arrays\n",
    "    # Each array is (n_samples, n_features)\n",
    "    if isinstance(shap_values, list):\n",
    "        # Get SHAP values for the predicted class\n",
    "        clean_shap = shap_values[int(clean_pred)][i]\n",
    "        adv_shap = shap_values_adv[int(adv_pred)][i]\n",
    "    else:\n",
    "        # Single array case\n",
    "        clean_shap = shap_values[i]\n",
    "        adv_shap = shap_values_adv[i]\n",
    "    \n",
    "    # Ensure we have 1D arrays\n",
    "    clean_shap = np.atleast_1d(np.array(clean_shap))\n",
    "    adv_shap = np.atleast_1d(np.array(adv_shap))\n",
    "    \n",
    "    explanation_change = np.linalg.norm(clean_shap - adv_shap)\n",
    "    explanation_changes.append(explanation_change)\n",
    "    \n",
    "    print(f\"  Explanation change (L2): {explanation_change:.3f}\")\n",
    "    \n",
    "    # Get top feature indices (flatten to ensure 1D)\n",
    "    clean_shap_flat = np.abs(clean_shap).flatten()\n",
    "    adv_shap_flat = np.abs(adv_shap).flatten()\n",
    "    \n",
    "    # Get top 2 features (or fewer if not enough features)\n",
    "    n_top = min(2, len(clean_shap_flat))\n",
    "    clean_top_idx = np.argsort(clean_shap_flat)[-n_top:][::-1]\n",
    "    adv_top_idx = np.argsort(adv_shap_flat)[-n_top:][::-1]\n",
    "    \n",
    "    # Build feature lists safely (ensure indices are valid)\n",
    "    clean_top_features = []\n",
    "    for idx in clean_top_idx:\n",
    "        idx_int = int(idx)\n",
    "        if 0 <= idx_int < len(feature_names):\n",
    "            clean_top_features.append(feature_names[idx_int])\n",
    "    \n",
    "    adv_top_features = []\n",
    "    for idx in adv_top_idx:\n",
    "        idx_int = int(idx)\n",
    "        if 0 <= idx_int < len(feature_names):\n",
    "            adv_top_features.append(feature_names[idx_int])\n",
    "    \n",
    "    print(f\"  Clean top features: {clean_top_features}\")\n",
    "    print(f\"  Adversarial top features: {adv_top_features}\")\n",
    "    \n",
    "    if list(clean_top_idx) == list(adv_top_idx):\n",
    "        print(f\"  Feature importance order: ✓ Stable\")\n",
    "    else:\n",
    "        print(f\"  Feature importance order: ✗ Changed\")\n",
    "\n",
    "# Statistical analysis\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STATISTICAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nExplanation Changes:\")\n",
    "print(f\"  Mean change: {np.mean(explanation_changes):.3f}\")\n",
    "print(f\"  Max change: {np.max(explanation_changes):.3f}\")\n",
    "print(f\"  Std dev: {np.std(explanation_changes):.3f}\")\n",
    "\n",
    "# Calculate correlation\n",
    "all_clean_shap = []\n",
    "all_adv_shap = []\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    clean_pred = clean_preds[i]\n",
    "    adv_pred = adv_preds[i]\n",
    "    \n",
    "    # Get SHAP values with proper indexing\n",
    "    if isinstance(shap_values, list):\n",
    "        all_clean_shap.append(shap_values[int(clean_pred)][i])\n",
    "        all_adv_shap.append(shap_values_adv[int(adv_pred)][i])\n",
    "    else:\n",
    "        all_clean_shap.append(shap_values[i])\n",
    "        all_adv_shap.append(shap_values_adv[i])\n",
    "\n",
    "all_clean_shap = np.array(all_clean_shap).flatten()\n",
    "all_adv_shap = np.array(all_adv_shap).flatten()\n",
    "\n",
    "correlation = np.corrcoef(all_clean_shap, all_adv_shap)[0, 1]\n",
    "\n",
    "print(f\"\\nExplanation Correlation:\")\n",
    "print(f\"  Correlation coefficient: {correlation:.3f}\")\n",
    "\n",
    "if correlation > 0.8:\n",
    "    print(f\"  ✓ Explanations are relatively stable\")\n",
    "elif correlation > 0.5:\n",
    "    print(f\"  ⚠ Explanations show moderate instability\")\n",
    "else:\n",
    "    print(f\"  ✗ Explanations are highly unstable\")\n",
    "\n",
    "# Key findings\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY FINDINGS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. Explanation Robustness:\")\n",
    "if np.mean(explanation_changes) < 1.0:\n",
    "    print(\"   ✓ SHAP explanations are relatively stable under perturbation\")\n",
    "else:\n",
    "    print(\"   ✗ SHAP explanations change significantly under adversarial perturbation\")\n",
    "\n",
    "print(\"\\n2. Security Implications:\")\n",
    "print(\"   - Adversarial examples can fool both predictions AND explanations\")\n",
    "print(\"   - Cannot rely on explanations alone for security\")\n",
    "print(\"   - Need robust models, not just interpretable ones\")\n",
    "\n",
    "print(\"\\n3. Comparison with Alibi:\")\n",
    "print(\"   - SHAP provides feature importance (global explanations)\")\n",
    "print(\"   - Alibi Anchors provide rule-based explanations (local explanations)\")\n",
    "print(\"   - Both can be affected by adversarial perturbations\")\n",
    "print(\"   - SHAP is more compatible with newer Python versions\")\n",
    "\n",
    "print(\"\\n4. Recommendations:\")\n",
    "print(\"   - Use adversarial training to improve robustness\")\n",
    "print(\"   - Combine multiple explanation methods\")\n",
    "print(\"   - Monitor explanation stability in production\")\n",
    "print(\"   - Implement input validation and anomaly detection\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ Lab 3 Complete!\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Key Insights**:\n",
    "\n",
    "1. **Explanation Fragility**: Explanations change with small perturbations\n",
    "2. **Trust Score Sensitivity**: Trust scores can detect adversarial examples\n",
    "3. **Security Gap**: Interpretability ≠ Security\n",
    "4. **Defense Strategy**: Need robust models + robust explanations\n",
    "\n",
    "---\n",
    "\n",
    "## Lab 4: Comprehensive Assessment - Exercise Answer\n",
    "\n",
    "### Exercise: Extend Assessment Framework\n",
    "\n",
    "**Task**: Add privacy tests, fairness evaluation, multiple defense strategies, and automated remediation suggestions.\n",
    "\n",
    "**Answer**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "COMPREHENSIVE SECURITY ASSESSMENT\n",
      "================================================================================\n",
      "Running evasion tests...\n",
      "Running privacy tests...\n",
      "Running fairness tests...\n",
      "Testing defense strategies...\n",
      "  Testing adversarial_training...\n",
      "  Testing input_transformation...\n",
      "  Testing ensemble...\n",
      "\n",
      "Generating remediation report...\n",
      "\n",
      "================================================================================\n",
      "REMEDIATION RECOMMENDATIONS\n",
      "================================================================================\n",
      "\n",
      "✓ Report exported to security_assessment.json\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "class ComprehensiveSecurityAssessment:\n",
    "    \"\"\"\n",
    "    Extended security assessment framework with privacy, fairness, and defenses.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, model, test_data, test_labels):\n",
    "        self.model = model\n",
    "        self.test_data = test_data\n",
    "        self.test_labels = test_labels\n",
    "        self.results = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'tests': {}\n",
    "        }\n",
    "    \n",
    "    def run_evasion_tests(self):\n",
    "        \"\"\"Test adversarial robustness\"\"\"\n",
    "        print(\"Running evasion tests...\")\n",
    "        \n",
    "        # FGSM attack\n",
    "        epsilon_values = [0.01, 0.05, 0.1]\n",
    "        fgsm_results = []\n",
    "        \n",
    "        for eps in epsilon_values:\n",
    "            adv_data = self._fgsm_attack(self.test_data, eps)\n",
    "            adv_acc = self._evaluate_accuracy(adv_data, self.test_labels)\n",
    "            fgsm_results.append({\n",
    "                'epsilon': eps,\n",
    "                'accuracy': adv_acc\n",
    "            })\n",
    "        \n",
    "        self.results['tests']['evasion'] = {\n",
    "            'fgsm': fgsm_results,\n",
    "            'baseline_accuracy': self._evaluate_accuracy(\n",
    "                self.test_data, self.test_labels\n",
    "            )\n",
    "        }\n",
    "    \n",
    "    def run_privacy_tests(self):\n",
    "        \"\"\"Test privacy vulnerabilities\"\"\"\n",
    "        print(\"Running privacy tests...\")\n",
    "        \n",
    "        # Membership inference\n",
    "        member_confidence = self._get_prediction_confidence(\n",
    "            self.test_data[:50]  # Assume these are members\n",
    "        )\n",
    "        non_member_confidence = self._get_prediction_confidence(\n",
    "            self.test_data[50:100]  # Assume these are non-members\n",
    "        )\n",
    "        \n",
    "        # Simple threshold-based attack\n",
    "        threshold = (member_confidence.mean() + non_member_confidence.mean()) / 2\n",
    "        member_detected = (member_confidence > threshold).float().mean()\n",
    "        non_member_detected = (non_member_confidence <= threshold).float().mean()\n",
    "        \n",
    "        privacy_score = (member_detected + non_member_detected) / 2\n",
    "        \n",
    "        self.results['tests']['privacy'] = {\n",
    "            'membership_inference': {\n",
    "                'attack_accuracy': privacy_score.item(),\n",
    "                'member_detection_rate': member_detected.item(),\n",
    "                'non_member_detection_rate': non_member_detected.item(),\n",
    "                'risk_level': 'HIGH' if privacy_score > 0.7 else 'MEDIUM' if privacy_score > 0.6 else 'LOW'\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def run_fairness_tests(self, sensitive_attribute):\n",
    "        \"\"\"Test fairness across groups\"\"\"\n",
    "        print(\"Running fairness tests...\")\n",
    "        \n",
    "        # Split by sensitive attribute\n",
    "        group_0_mask = (sensitive_attribute == 0)\n",
    "        group_1_mask = (sensitive_attribute == 1)\n",
    "        \n",
    "        # Accuracy by group\n",
    "        acc_group_0 = self._evaluate_accuracy(\n",
    "            self.test_data[group_0_mask],\n",
    "            self.test_labels[group_0_mask]\n",
    "        )\n",
    "        acc_group_1 = self._evaluate_accuracy(\n",
    "            self.test_data[group_1_mask],\n",
    "            self.test_labels[group_1_mask]\n",
    "        )\n",
    "        \n",
    "        # Demographic parity\n",
    "        pred_group_0 = self.model(self.test_data[group_0_mask]).argmax(dim=1)\n",
    "        pred_group_1 = self.model(self.test_data[group_1_mask]).argmax(dim=1)\n",
    "        \n",
    "        positive_rate_0 = (pred_group_0 == 1).float().mean()\n",
    "        positive_rate_1 = (pred_group_1 == 1).float().mean()\n",
    "        \n",
    "        demographic_parity_diff = abs(positive_rate_0 - positive_rate_1).item()\n",
    "        \n",
    "        self.results['tests']['fairness'] = {\n",
    "            'accuracy_group_0': acc_group_0,\n",
    "            'accuracy_group_1': acc_group_1,\n",
    "            'accuracy_disparity': abs(acc_group_0 - acc_group_1),\n",
    "            'demographic_parity_difference': demographic_parity_diff,\n",
    "            'fairness_level': 'FAIR' if demographic_parity_diff < 0.1 else 'UNFAIR'\n",
    "        }\n",
    "    \n",
    "    def test_defenses(self):\n",
    "        \"\"\"Test multiple defense strategies\"\"\"\n",
    "        print(\"Testing defense strategies...\")\n",
    "        \n",
    "        defenses = {\n",
    "            'adversarial_training': self._test_adversarial_training,\n",
    "            'input_transformation': self._test_input_transformation,\n",
    "            'ensemble': self._test_ensemble_defense\n",
    "        }\n",
    "        \n",
    "        defense_results = {}\n",
    "        for name, test_func in defenses.items():\n",
    "            print(f\"  Testing {name}...\")\n",
    "            defense_results[name] = test_func()\n",
    "        \n",
    "        self.results['tests']['defenses'] = defense_results\n",
    "    \n",
    "    def generate_remediation_report(self):\n",
    "        \"\"\"Generate automated remediation suggestions\"\"\"\n",
    "        print(\"\\nGenerating remediation report...\")\n",
    "        \n",
    "        recommendations = []\n",
    "        \n",
    "        # Evasion recommendations\n",
    "        if 'evasion' in self.results['tests']:\n",
    "            baseline = self.results['tests']['evasion']['baseline_accuracy']\n",
    "            worst_adv = min([r['accuracy'] for r in self.results['tests']['evasion']['fgsm']])\n",
    "            \n",
    "            if baseline - worst_adv > 0.2:\n",
    "                recommendations.append({\n",
    "                    'category': 'Evasion',\n",
    "                    'severity': 'HIGH',\n",
    "                    'issue': f'Model accuracy drops {(baseline-worst_adv)*100:.1f}% under FGSM attack',\n",
    "                    'remediation': [\n",
    "                        'Implement adversarial training with PGD',\n",
    "                        'Add input preprocessing (JPEG compression, bit depth reduction)',\n",
    "                        'Use certified defenses (randomized smoothing)',\n",
    "                        'Deploy ensemble of diverse models'\n",
    "                    ]\n",
    "                })\n",
    "        \n",
    "        # Privacy recommendations\n",
    "        if 'privacy' in self.results['tests']:\n",
    "            privacy_risk = self.results['tests']['privacy']['membership_inference']['risk_level']\n",
    "            \n",
    "            if privacy_risk in ['HIGH', 'MEDIUM']:\n",
    "                recommendations.append({\n",
    "                    'category': 'Privacy',\n",
    "                    'severity': privacy_risk,\n",
    "                    'issue': 'Model vulnerable to membership inference attacks',\n",
    "                    'remediation': [\n",
    "                        'Apply differential privacy during training (DP-SGD)',\n",
    "                        'Reduce model capacity to prevent overfitting',\n",
    "                        'Add prediction confidence masking',\n",
    "                        'Implement query rate limiting'\n",
    "                    ]\n",
    "                })\n",
    "        \n",
    "        # Fairness recommendations\n",
    "        if 'fairness' in self.results['tests']:\n",
    "            if self.results['tests']['fairness']['fairness_level'] == 'UNFAIR':\n",
    "                recommendations.append({\n",
    "                    'category': 'Fairness',\n",
    "                    'severity': 'MEDIUM',\n",
    "                    'issue': 'Significant disparity in model performance across groups',\n",
    "                    'remediation': [\n",
    "                        'Rebalance training data across groups',\n",
    "                        'Apply fairness constraints during training',\n",
    "                        'Use post-processing calibration',\n",
    "                        'Implement group-specific thresholds'\n",
    "                    ]\n",
    "                })\n",
    "        \n",
    "        self.results['remediation'] = recommendations\n",
    "        return recommendations\n",
    "    \n",
    "    def _fgsm_attack(self, data, epsilon):\n",
    "        \"\"\"Simple FGSM implementation\"\"\"\n",
    "        data_copy = data.clone().detach().requires_grad_(True)\n",
    "        outputs = self.model(data_copy)\n",
    "        loss = torch.nn.functional.cross_entropy(outputs, self.test_labels[:len(data)])\n",
    "        loss.backward()\n",
    "        \n",
    "        perturbation = epsilon * data_copy.grad.sign()\n",
    "        adv_data = data_copy + perturbation\n",
    "        return adv_data.detach()\n",
    "    \n",
    "    def _evaluate_accuracy(self, data, labels):\n",
    "        \"\"\"Evaluate model accuracy\"\"\"\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(data)\n",
    "            predictions = outputs.argmax(dim=1)\n",
    "            accuracy = (predictions == labels).float().mean().item()\n",
    "        return accuracy\n",
    "    \n",
    "    def _get_prediction_confidence(self, data):\n",
    "        \"\"\"Get prediction confidence scores\"\"\"\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model(data)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            confidence = probs.max(dim=1)[0]\n",
    "        return confidence\n",
    "    \n",
    "    def _test_adversarial_training(self):\n",
    "        \"\"\"Test adversarial training defense\"\"\"\n",
    "        # Simplified - would need actual adversarial training\n",
    "        return {\n",
    "            'effectiveness': 0.7,\n",
    "            'overhead': 'High (2-3x training time)',\n",
    "            'recommendation': 'Implement for production models'\n",
    "        }\n",
    "    \n",
    "    def _test_input_transformation(self):\n",
    "        \"\"\"Test input transformation defense\"\"\"\n",
    "        return {\n",
    "            'effectiveness': 0.5,\n",
    "            'overhead': 'Low',\n",
    "            'recommendation': 'Use as first line of defense'\n",
    "        }\n",
    "    \n",
    "    def _test_ensemble_defense(self):\n",
    "        \"\"\"Test ensemble defense\"\"\"\n",
    "        return {\n",
    "            'effectiveness': 0.6,\n",
    "            'overhead': 'Medium (N models)',\n",
    "            'recommendation': 'Good for critical applications'\n",
    "        }\n",
    "    \n",
    "    def export_report(self, filename='security_assessment.json'):\n",
    "        \"\"\"Export comprehensive report\"\"\"\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(self.results, f, indent=2)\n",
    "        print(f\"\\n✓ Report exported to {filename}\")\n",
    "\n",
    "# Example usage\n",
    "class SimpleModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = torch.nn.Sequential(\n",
    "            torch.nn.Flatten(),\n",
    "            torch.nn.Linear(784, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, 10)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "# Create test data\n",
    "test_data = torch.randn(100, 1, 28, 28)\n",
    "test_labels = torch.randint(0, 10, (100,))\n",
    "sensitive_attr = torch.randint(0, 2, (100,))\n",
    "\n",
    "# Initialize model\n",
    "model = SimpleModel()\n",
    "model.eval()\n",
    "\n",
    "# Run comprehensive assessment\n",
    "print(\"=\"*80)\n",
    "print(\"COMPREHENSIVE SECURITY ASSESSMENT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "assessment = ComprehensiveSecurityAssessment(model, test_data, test_labels)\n",
    "assessment.run_evasion_tests()\n",
    "assessment.run_privacy_tests()\n",
    "assessment.run_fairness_tests(sensitive_attr)\n",
    "assessment.test_defenses()\n",
    "\n",
    "# Generate remediation report\n",
    "recommendations = assessment.generate_remediation_report()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"REMEDIATION RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"\\n{i}. {rec['category']} - Severity: {rec['severity']}\")\n",
    "    print(f\"   Issue: {rec['issue']}\")\n",
    "    print(f\"   Remediation:\")\n",
    "    for action in rec['remediation']:\n",
    "        print(f\"     • {action}\")\n",
    "\n",
    "# Export report\n",
    "assessment.export_report()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Framework Features**:\n",
    "1. **Multi-dimensional Testing**: Evasion, privacy, fairness\n",
    "2. **Defense Evaluation**: Test multiple defense strategies\n",
    "3. **Automated Remediation**: Generate actionable recommendations\n",
    "4. **Comprehensive Reporting**: JSON export for documentation\n",
    "\n",
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "Module 7 exercises demonstrate:\n",
    "- Comparing attack methods reveals trade-offs\n",
    "- Explanations are not robust to adversarial examples\n",
    "- Comprehensive assessment requires multiple test dimensions\n",
    "- Automated remediation guides defense implementation\n",
    "\n",
    "Course complete! Proceed to Module 8 Capstone Project!\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
