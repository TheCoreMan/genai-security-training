{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Key Findings**:\n",
    "\n",
    "1. **BAE (BERT-based Adversarial Examples)**:\n",
    "   - Uses BERT masked language model\n",
    "   - High semantic similarity\n",
    "   - Moderate success rate\n",
    "   - More queries needed\n",
    "\n",
    "2. **PWWS (Probability Weighted Word Saliency)**:\n",
    "   - Uses word importance scores\n",
    "   - High success rate\n",
    "   - May sacrifice some naturalness\n",
    "   - Efficient (fewer queries)\n",
    "\n",
    "3. **TextFooler**:\n",
    "   - Balanced approach\n",
    "   - Good semantic similarity\n",
    "   - Reasonable success rate\n",
    "   - General-purpose attack\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Lab 3: SHAP Explainability - Exercise\n",
    "\n",
    "### Exercise: Test Explanation Robustness\n",
    "\n",
    "**Task**: Test explanation robustness on adversarial examples. Compare trust scores for clean vs adversarial samples.\n",
    "\n",
    "**Your Task**:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Libraries loaded successfully\n",
      "\n",
      "Model accuracy: 100.00%\n",
      "\n",
      "Creating SHAP explainer...\n",
      "✓ SHAP explainer created\n",
      "\n",
      "Generating adversarial examples...\n",
      "Adversarial success rate: 6.7%\n",
      "Adversarial accuracy: 93.33%\n",
      "Accuracy drop: 6.67%\n",
      "\n",
      "Computing SHAP values for adversarial examples...\n",
      "\n",
      "================================================================================\n",
      "EXPLANATION ROBUSTNESS ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "Sample 1:\n",
      "  Clean prediction: versicolor\n",
      "  Adversarial prediction: versicolor\n",
      "  Prediction changed: False\n",
      "  Explanation change (L2): 0.588\n",
      "  Clean top features: []\n",
      "  Adversarial top features: []\n",
      "  Feature importance order: ✗ Changed\n",
      "\n",
      "Sample 2:\n",
      "  Clean prediction: setosa\n",
      "  Adversarial prediction: setosa\n",
      "  Prediction changed: False\n",
      "  Explanation change (L2): 0.057\n",
      "  Clean top features: []\n",
      "  Adversarial top features: []\n",
      "  Feature importance order: ✓ Stable\n",
      "\n",
      "Sample 3:\n",
      "  Clean prediction: virginica\n",
      "  Adversarial prediction: virginica\n",
      "  Prediction changed: False\n",
      "  Explanation change (L2): 0.026\n",
      "  Clean top features: []\n",
      "  Adversarial top features: []\n",
      "  Feature importance order: ✓ Stable\n",
      "\n",
      "Sample 4:\n",
      "  Clean prediction: versicolor\n",
      "  Adversarial prediction: versicolor\n",
      "  Prediction changed: False\n",
      "  Explanation change (L2): 0.017\n",
      "  Clean top features: []\n",
      "  Adversarial top features: []\n",
      "  Feature importance order: ✓ Stable\n",
      "\n",
      "Sample 5:\n",
      "  Clean prediction: versicolor\n",
      "  Adversarial prediction: versicolor\n",
      "  Prediction changed: False\n",
      "  Explanation change (L2): 0.268\n",
      "  Clean top features: []\n",
      "  Adversarial top features: []\n",
      "  Feature importance order: ✗ Changed\n",
      "\n",
      "================================================================================\n",
      "STATISTICAL SUMMARY\n",
      "================================================================================\n",
      "\n",
      "Explanation Changes:\n",
      "  Mean change: 0.191\n",
      "  Max change: 0.588\n",
      "  Std dev: 0.219\n",
      "\n",
      "Explanation Correlation:\n",
      "  Correlation coefficient: 0.901\n",
      "  ✓ Explanations are relatively stable\n",
      "\n",
      "================================================================================\n",
      "KEY FINDINGS\n",
      "================================================================================\n",
      "\n",
      "1. Explanation Robustness:\n",
      "   ✓ SHAP explanations are relatively stable under perturbation\n",
      "\n",
      "2. Security Implications:\n",
      "   - Adversarial examples can fool both predictions AND explanations\n",
      "   - Cannot rely on explanations alone for security\n",
      "   - Need robust models, not just interpretable ones\n",
      "\n",
      "3. Comparison with Alibi:\n",
      "   - SHAP provides feature importance (global explanations)\n",
      "   - Alibi Anchors provide rule-based explanations (local explanations)\n",
      "   - Both can be affected by adversarial perturbations\n",
      "   - SHAP is more compatible with newer Python versions\n",
      "\n",
      "4. Recommendations:\n",
      "   - Use adversarial training to improve robustness\n",
      "   - Combine multiple explanation methods\n",
      "   - Monitor explanation stability in production\n",
      "   - Implement input validation and anomaly detection\n",
      "\n",
      "================================================================================\n",
      "✓ Lab 3 Complete!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Lab 3: Explanation Robustness Testing (Python 3.14 Compatible)\n",
    "# Note: Using SHAP instead of Alibi due to Python 3.14 compatibility\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Install shap if needed\n",
    "try:\n",
    "    import shap\n",
    "except ImportError:\n",
    "    print(\"Installing shap...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call(['pip', 'install', '-q', 'shap==0.45.0'])\n",
    "    import shap\n",
    "\n",
    "print(\"✓ Libraries loaded successfully\")\n",
    "\n",
    "# Load dataset\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train model\n",
    "model = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "baseline_accuracy = model.score(X_test, y_test)\n",
    "print(f\"\\nModel accuracy: {baseline_accuracy:.2%}\")\n",
    "\n",
    "# Feature names\n",
    "feature_names = ['sepal length', 'sepal width', 'petal length', 'petal width']\n",
    "class_names = ['setosa', 'versicolor', 'virginica']\n",
    "\n",
    "# Create SHAP explainer\n",
    "print(\"\\nCreating SHAP explainer...\")\n",
    "explainer = shap.TreeExplainer(model)\n",
    "\n",
    "# Get SHAP values for test samples\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "print(\"✓ SHAP explainer created\")\n",
    "\n",
    "# Function to create adversarial examples\n",
    "def create_adversarial_tabular(X, model, epsilon=0.3):\n",
    "    X_adv = X.copy()\n",
    "    for i in range(len(X)):\n",
    "        perturbation = np.random.randn(X.shape[1]) * epsilon\n",
    "        X_adv[i] += perturbation\n",
    "        X_adv[i] = np.maximum(X_adv[i], 0)\n",
    "    return X_adv\n",
    "\n",
    "# Generate adversarial examples\n",
    "print(\"\\nGenerating adversarial examples...\")\n",
    "X_test_adv = create_adversarial_tabular(X_test, model, epsilon=0.3)\n",
    "\n",
    "# Verify some are adversarial\n",
    "clean_preds = model.predict(X_test)\n",
    "adv_preds = model.predict(X_test_adv)\n",
    "adv_success_rate = (clean_preds != adv_preds).mean()\n",
    "adv_accuracy = accuracy_score(y_test, adv_preds)\n",
    "\n",
    "print(f\"Adversarial success rate: {adv_success_rate:.1%}\")\n",
    "print(f\"Adversarial accuracy: {adv_accuracy:.2%}\")\n",
    "print(f\"Accuracy drop: {(baseline_accuracy - adv_accuracy):.2%}\")\n",
    "\n",
    "# Get SHAP values for adversarial examples\n",
    "print(\"\\nComputing SHAP values for adversarial examples...\")\n",
    "shap_values_adv = explainer.shap_values(X_test_adv)\n",
    "\n",
    "# Compare explanation stability\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPLANATION ROBUSTNESS ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "num_samples = min(5, len(X_test))\n",
    "explanation_changes = []\n",
    "\n",
    "for i in range(num_samples):\n",
    "    clean_sample = X_test[i]\n",
    "    adv_sample = X_test_adv[i]\n",
    "    clean_pred = clean_preds[i]\n",
    "    adv_pred = adv_preds[i]\n",
    "    \n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(f\"  Clean prediction: {class_names[clean_pred]}\")\n",
    "    print(f\"  Adversarial prediction: {class_names[adv_pred]}\")\n",
    "    print(f\"  Prediction changed: {clean_pred != adv_pred}\")\n",
    "    \n",
    "    # Compare SHAP values (for multi-class, use the predicted class)\n",
    "    # For RandomForest with 3 classes, shap_values is a list of 3 arrays\n",
    "    # Each array is (n_samples, n_features)\n",
    "    if isinstance(shap_values, list):\n",
    "        # Get SHAP values for the predicted class\n",
    "        clean_shap = shap_values[int(clean_pred)][i]\n",
    "        adv_shap = shap_values_adv[int(adv_pred)][i]\n",
    "    else:\n",
    "        # Single array case\n",
    "        clean_shap = shap_values[i]\n",
    "        adv_shap = shap_values_adv[i]\n",
    "    \n",
    "    # Ensure we have 1D arrays\n",
    "    clean_shap = np.atleast_1d(np.array(clean_shap))\n",
    "    adv_shap = np.atleast_1d(np.array(adv_shap))\n",
    "    \n",
    "    explanation_change = np.linalg.norm(clean_shap - adv_shap)\n",
    "    explanation_changes.append(explanation_change)\n",
    "    \n",
    "    print(f\"  Explanation change (L2): {explanation_change:.3f}\")\n",
    "    \n",
    "    # Get top feature indices (flatten to ensure 1D)\n",
    "    clean_shap_flat = np.abs(clean_shap).flatten()\n",
    "    adv_shap_flat = np.abs(adv_shap).flatten()\n",
    "    \n",
    "    # Get top 2 features (or fewer if not enough features)\n",
    "    n_top = min(2, len(clean_shap_flat))\n",
    "    clean_top_idx = np.argsort(clean_shap_flat)[-n_top:][::-1]\n",
    "    adv_top_idx = np.argsort(adv_shap_flat)[-n_top:][::-1]\n",
    "    \n",
    "    # Build feature lists safely (ensure indices are valid)\n",
    "    clean_top_features = []\n",
    "    for idx in clean_top_idx:\n",
    "        idx_int = int(idx)\n",
    "        if 0 <= idx_int < len(feature_names):\n",
    "            clean_top_features.append(feature_names[idx_int])\n",
    "    \n",
    "    adv_top_features = []\n",
    "    for idx in adv_top_idx:\n",
    "        idx_int = int(idx)\n",
    "        if 0 <= idx_int < len(feature_names):\n",
    "            adv_top_features.append(feature_names[idx_int])\n",
    "    \n",
    "    print(f\"  Clean top features: {clean_top_features}\")\n",
    "    print(f\"  Adversarial top features: {adv_top_features}\")\n",
    "    \n",
    "    if list(clean_top_idx) == list(adv_top_idx):\n",
    "        print(f\"  Feature importance order: ✓ Stable\")\n",
    "    else:\n",
    "        print(f\"  Feature importance order: ✗ Changed\")\n",
    "\n",
    "# Statistical analysis\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"STATISTICAL SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nExplanation Changes:\")\n",
    "print(f\"  Mean change: {np.mean(explanation_changes):.3f}\")\n",
    "print(f\"  Max change: {np.max(explanation_changes):.3f}\")\n",
    "print(f\"  Std dev: {np.std(explanation_changes):.3f}\")\n",
    "\n",
    "# Calculate correlation\n",
    "all_clean_shap = []\n",
    "all_adv_shap = []\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    clean_pred = clean_preds[i]\n",
    "    adv_pred = adv_preds[i]\n",
    "    \n",
    "    # Get SHAP values with proper indexing\n",
    "    if isinstance(shap_values, list):\n",
    "        all_clean_shap.append(shap_values[int(clean_pred)][i])\n",
    "        all_adv_shap.append(shap_values_adv[int(adv_pred)][i])\n",
    "    else:\n",
    "        all_clean_shap.append(shap_values[i])\n",
    "        all_adv_shap.append(shap_values_adv[i])\n",
    "\n",
    "all_clean_shap = np.array(all_clean_shap).flatten()\n",
    "all_adv_shap = np.array(all_adv_shap).flatten()\n",
    "\n",
    "correlation = np.corrcoef(all_clean_shap, all_adv_shap)[0, 1]\n",
    "\n",
    "print(f\"\\nExplanation Correlation:\")\n",
    "print(f\"  Correlation coefficient: {correlation:.3f}\")\n",
    "\n",
    "if correlation > 0.8:\n",
    "    print(f\"  ✓ Explanations are relatively stable\")\n",
    "elif correlation > 0.5:\n",
    "    print(f\"  ⚠ Explanations show moderate instability\")\n",
    "else:\n",
    "    print(f\"  ✗ Explanations are highly unstable\")\n",
    "\n",
    "# Key findings\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY FINDINGS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. Explanation Robustness:\")\n",
    "if np.mean(explanation_changes) < 1.0:\n",
    "    print(\"   ✓ SHAP explanations are relatively stable under perturbation\")\n",
    "else:\n",
    "    print(\"   ✗ SHAP explanations change significantly under adversarial perturbation\")\n",
    "\n",
    "print(\"\\n2. Security Implications:\")\n",
    "print(\"   - Adversarial examples can fool both predictions AND explanations\")\n",
    "print(\"   - Cannot rely on explanations alone for security\")\n",
    "print(\"   - Need robust models, not just interpretable ones\")\n",
    "\n",
    "print(\"\\n3. Comparison with Alibi:\")\n",
    "print(\"   - SHAP provides feature importance (global explanations)\")\n",
    "print(\"   - Alibi Anchors provide rule-based explanations (local explanations)\")\n",
    "print(\"   - Both can be affected by adversarial perturbations\")\n",
    "print(\"   - SHAP is more compatible with newer Python versions\")\n",
    "\n",
    "print(\"\\n4. Recommendations:\")\n",
    "print(\"   - Use adversarial training to improve robustness\")\n",
    "print(\"   - Combine multiple explanation methods\")\n",
    "print(\"   - Monitor explanation stability in production\")\n",
    "print(\"   - Implement input validation and anomaly detection\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"✓ Lab 3 Complete!\")\n",
    "print(\"=\"*80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
