{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Text Adversarial Attacks\n",
    "\n",
    "## Overview\n",
    "\n",
    "Text attacks face unique challenges compared to image attacks: discrete input space, semantic constraints, and human readability. In this lab, you'll implement various text attack techniques.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "1. Understand challenges of adversarial NLP\n",
    "2. Implement character-level attacks (homoglyphs, typos)\n",
    "3. Execute word-level attacks (synonyms, embeddings)\n",
    "4. Use TextAttack framework\n",
    "5. Evaluate semantic similarity\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "- Understanding of NLP basics\n",
    "- Familiarity with transformers\n",
    "- Completion of Labs 1-2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Install Required Packages\n",
    "\n",
    "This lab requires `textattack`. Run this cell to install it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/schwartz/src/genai-security-training/.venv/lib/python3.14/site-packages/jieba/_compat.py:18: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ TextAttack already installed\n"
     ]
    }
   ],
   "source": [
    "# Install textattack if not already installed\n",
    "try:\n",
    "    import textattack\n",
    "    print(\"✓ TextAttack already installed\")\n",
    "except ImportError:\n",
    "    print(\"Installing textattack...\")\n",
    "    !pip install -q textattack==0.3.10\n",
    "    print(\"✓ TextAttack installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Using Apple Silicon GPU (MPS)\n",
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import pipeline\n",
    "import random\n",
    "import string\n",
    "\n",
    "# For semantic similarity\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Detect device (supports CUDA, Apple Silicon MPS, and CPU)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    device_id = 0\n",
    "    print(f\"✓ Using CUDA GPU: {torch.cuda.get_device_name(0)}\")\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    device_id = -1  # MPS not supported by pipeline, will use CPU for pipeline\n",
    "    print(\"✓ Using Apple Silicon GPU (MPS)\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    device_id = -1\n",
    "    print(\"ℹ Using CPU\")\n",
    "\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Sentiment Analysis Model\n",
    "\n",
    "We'll attack a sentiment classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Models loaded\n"
     ]
    }
   ],
   "source": [
    "# Load sentiment analysis model\n",
    "model_name = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Create pipeline\n",
    "sentiment_pipeline = pipeline('sentiment-analysis', model=model, tokenizer=tokenizer, device=device_id)\n",
    "\n",
    "# Load semantic similarity model\n",
    "sim_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "print(\"✓ Models loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Original Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Predictions:\n",
      "\n",
      "Text: This movie is absolutely wonderful and amazing!\n",
      "Prediction: POSITIVE (99.99%)\n",
      "\n",
      "Text: I love this product, it works perfectly.\n",
      "Prediction: POSITIVE (99.99%)\n",
      "\n",
      "Text: This is the worst experience I've ever had.\n",
      "Prediction: NEGATIVE (99.98%)\n",
      "\n",
      "Text: Terrible service, would not recommend.\n",
      "Prediction: NEGATIVE (99.26%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test sentences\n",
    "test_sentences = [\n",
    "    \"This movie is absolutely wonderful and amazing!\",\n",
    "    \"I love this product, it works perfectly.\",\n",
    "    \"This is the worst experience I've ever had.\",\n",
    "    \"Terrible service, would not recommend.\"\n",
    "]\n",
    "\n",
    "print(\"Original Predictions:\\n\")\n",
    "for sent in test_sentences:\n",
    "    result = sentiment_pipeline(sent)[0]\n",
    "    print(f\"Text: {sent}\")\n",
    "    print(f\"Prediction: {result['label']} ({result['score']:.2%})\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Character-Level Attacks\n",
    "\n",
    "### Attack 1.1: Homoglyph Substitution\n",
    "\n",
    "Replace characters with visually similar Unicode characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:    This movie is absolutely wonderful and amazing!\n",
      "Adversarial: Thіs movіė іs аbsοlutėly wonderful ɑnd аmazing!\n",
      "\n",
      "Visually identical? False\n",
      "\n",
      "Original prediction: POSITIVE (99.99%)\n",
      "Adversarial prediction: POSITIVE (99.84%)\n",
      "Attack successful: False\n"
     ]
    }
   ],
   "source": [
    "def homoglyph_attack(text, substitution_rate=0.3):\n",
    "    \"\"\"\n",
    "    Replace characters with visually similar Unicode characters\n",
    "    \n",
    "    Args:\n",
    "        text: Input text\n",
    "        substitution_rate: Fraction of characters to replace\n",
    "    \n",
    "    Returns:\n",
    "        Adversarial text\n",
    "    \"\"\"\n",
    "    homoglyphs = {\n",
    "        'a': ['а', 'ɑ'],  # Cyrillic a, Latin alpha\n",
    "        'e': ['е', 'ė'],  # Cyrillic e, Latin e with dot\n",
    "        'o': ['о', 'ο'],  # Cyrillic o, Greek omicron\n",
    "        'i': ['і', 'ı'],  # Cyrillic i, dotless i\n",
    "        'c': ['с', 'ϲ'],  # Cyrillic s, Greek lunate sigma\n",
    "        'p': ['р'],       # Cyrillic r\n",
    "        'x': ['х'],       # Cyrillic h\n",
    "    }\n",
    "    \n",
    "    adversarial = list(text)\n",
    "    num_substitutions = int(len(text) * substitution_rate)\n",
    "    \n",
    "    # Get positions of replaceable characters\n",
    "    positions = [i for i, char in enumerate(text) if char.lower() in homoglyphs]\n",
    "    \n",
    "    if positions:\n",
    "        # Randomly select positions to replace\n",
    "        replace_positions = random.sample(positions, min(num_substitutions, len(positions)))\n",
    "        \n",
    "        for pos in replace_positions:\n",
    "            char = text[pos].lower()\n",
    "            if char in homoglyphs:\n",
    "                replacement = random.choice(homoglyphs[char])\n",
    "                # Preserve case\n",
    "                if text[pos].isupper():\n",
    "                    replacement = replacement.upper()\n",
    "                adversarial[pos] = replacement\n",
    "    \n",
    "    return ''.join(adversarial)\n",
    "\n",
    "# Test homoglyph attack\n",
    "original = \"This movie is absolutely wonderful and amazing!\"\n",
    "adversarial = homoglyph_attack(original, substitution_rate=0.2)\n",
    "\n",
    "print(f\"Original:    {original}\")\n",
    "print(f\"Adversarial: {adversarial}\")\n",
    "print(f\"\\nVisually identical? {original == adversarial}\")\n",
    "\n",
    "# Test on model\n",
    "orig_result = sentiment_pipeline(original)[0]\n",
    "adv_result = sentiment_pipeline(adversarial)[0]\n",
    "\n",
    "print(f\"\\nOriginal prediction: {orig_result['label']} ({orig_result['score']:.2%})\")\n",
    "print(f\"Adversarial prediction: {adv_result['label']} ({adv_result['score']:.2%})\")\n",
    "print(f\"Attack successful: {orig_result['label'] != adv_result['label']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attack 1.2: Character Insertion/Deletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  This movie is absolutely wonderful!\n",
      "Inserted:  This movie is absolutelryb wonderful!g\n",
      "Deleted:   This movie i abslutely wonderul!\n"
     ]
    }
   ],
   "source": [
    "def char_insertion_attack(text, insertion_rate=0.1):\n",
    "    \"\"\"\n",
    "    Insert random characters to evade detection\n",
    "    \"\"\"\n",
    "    adversarial = list(text)\n",
    "    num_insertions = int(len(text) * insertion_rate)\n",
    "    \n",
    "    for _ in range(num_insertions):\n",
    "        pos = random.randint(0, len(adversarial))\n",
    "        char = random.choice(string.ascii_lowercase)\n",
    "        adversarial.insert(pos, char)\n",
    "    \n",
    "    return ''.join(adversarial)\n",
    "\n",
    "def char_deletion_attack(text, deletion_rate=0.1):\n",
    "    \"\"\"\n",
    "    Delete random characters\n",
    "    \"\"\"\n",
    "    adversarial = list(text)\n",
    "    num_deletions = int(len(text) * deletion_rate)\n",
    "    \n",
    "    # Get positions of non-space characters\n",
    "    positions = [i for i, char in enumerate(adversarial) if char != ' ']\n",
    "    \n",
    "    if positions:\n",
    "        delete_positions = random.sample(positions, min(num_deletions, len(positions)))\n",
    "        # Delete in reverse order to maintain indices\n",
    "        for pos in sorted(delete_positions, reverse=True):\n",
    "            del adversarial[pos]\n",
    "    \n",
    "    return ''.join(adversarial)\n",
    "\n",
    "# Test\n",
    "original = \"This movie is absolutely wonderful!\"\n",
    "inserted = char_insertion_attack(original, 0.1)\n",
    "deleted = char_deletion_attack(original, 0.1)\n",
    "\n",
    "print(f\"Original:  {original}\")\n",
    "print(f\"Inserted:  {inserted}\")\n",
    "print(f\"Deleted:   {deleted}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Word-Level Attacks\n",
    "\n",
    "### Attack 2.1: Synonym Substitution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:    This movie is absolutely wonderful and amazing!\n",
      "Adversarial: This movie is absolutely marvelous and outstanding!\n",
      "\n",
      "Semantic similarity: 89.08%\n",
      "\n",
      "Original: POSITIVE (99.99%)\n",
      "Adversarial: POSITIVE (99.99%)\n"
     ]
    }
   ],
   "source": [
    "# Simple synonym dictionary (in practice, use WordNet or BERT)\n",
    "SYNONYMS = {\n",
    "    'wonderful': ['great', 'excellent', 'fantastic', 'marvelous'],\n",
    "    'amazing': ['incredible', 'awesome', 'outstanding', 'remarkable'],\n",
    "    'terrible': ['awful', 'horrible', 'dreadful', 'atrocious'],\n",
    "    'worst': ['poorest', 'most terrible', 'most awful'],\n",
    "    'love': ['adore', 'enjoy', 'appreciate'],\n",
    "    'hate': ['despise', 'detest', 'loathe'],\n",
    "}\n",
    "\n",
    "def synonym_attack(text, substitution_rate=0.5):\n",
    "    \"\"\"\n",
    "    Replace words with synonyms\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    adversarial = words.copy()\n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        # Remove punctuation for lookup\n",
    "        clean_word = word.strip(string.punctuation).lower()\n",
    "        \n",
    "        if clean_word in SYNONYMS and random.random() < substitution_rate:\n",
    "            synonym = random.choice(SYNONYMS[clean_word])\n",
    "            # Preserve punctuation and case\n",
    "            if word[0].isupper():\n",
    "                synonym = synonym.capitalize()\n",
    "            if word[-1] in string.punctuation:\n",
    "                synonym += word[-1]\n",
    "            adversarial[i] = synonym\n",
    "    \n",
    "    return ' '.join(adversarial)\n",
    "\n",
    "# Test synonym attack\n",
    "original = \"This movie is absolutely wonderful and amazing!\"\n",
    "adversarial = synonym_attack(original, substitution_rate=0.8)\n",
    "\n",
    "print(f\"Original:    {original}\")\n",
    "print(f\"Adversarial: {adversarial}\")\n",
    "\n",
    "# Calculate semantic similarity\n",
    "orig_embedding = sim_model.encode(original, convert_to_tensor=True)\n",
    "adv_embedding = sim_model.encode(adversarial, convert_to_tensor=True)\n",
    "similarity = util.cos_sim(orig_embedding, adv_embedding).item()\n",
    "\n",
    "print(f\"\\nSemantic similarity: {similarity:.2%}\")\n",
    "\n",
    "# Test on model\n",
    "orig_result = sentiment_pipeline(original)[0]\n",
    "adv_result = sentiment_pipeline(adversarial)[0]\n",
    "\n",
    "print(f\"\\nOriginal: {orig_result['label']} ({orig_result['score']:.2%})\")\n",
    "print(f\"Adversarial: {adv_result['label']} ({adv_result['score']:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Using TextAttack Framework\n",
    "\n",
    "TextAttack provides state-of-the-art text attack implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ TextAttack available\n"
     ]
    }
   ],
   "source": [
    "# Install TextAttack if needed\n",
    "# !pip install textattack\n",
    "\n",
    "try:\n",
    "    from textattack.attack_recipes import TextFoolerJin2019\n",
    "    from textattack.models.wrappers import HuggingFaceModelWrapper\n",
    "    from textattack.datasets import HuggingFaceDataset\n",
    "    from textattack import Attacker, AttackArgs\n",
    "    \n",
    "    print(\"✓ TextAttack available\")\n",
    "    textattack_available = True\n",
    "except ImportError:\n",
    "    print(\"⚠ TextAttack not installed. Install with: pip install textattack\")\n",
    "    textattack_available = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "textattack: Downloading https://textattack.s3.amazonaws.com/word_embeddings/paragramcf.\n",
      "100%|██████████| 481M/481M [04:32<00:00, 1.77MB/s] \n",
      "textattack: Unzipping file /Users/schwartz/.cache/textattack/tmpptqtvlad.zip to /Users/schwartz/.cache/textattack/word_embeddings/paragramcf.\n",
      "textattack: Successfully saved word_embeddings/paragramcf to cache.\n",
      "textattack: Unknown if model of class <class 'transformers.models.distilbert.modeling_distilbert.DistilBertForSequenceClassification'> compatible with goal function <class 'textattack.goal_functions.classification.untargeted_classification.UntargetedClassification'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: This movie is absolutely wonderful and amazing!\n",
      "\n",
      "Running TextFooler attack...\n"
     ]
    }
   ],
   "source": [
    "if textattack_available:\n",
    "    # Wrap model for TextAttack\n",
    "    model_wrapper = HuggingFaceModelWrapper(model, tokenizer)\n",
    "    \n",
    "    # Create TextFooler attack\n",
    "    attack = TextFoolerJin2019.build(model_wrapper)\n",
    "    \n",
    "    # Attack a single example\n",
    "    from textattack.shared import AttackedText\n",
    "    \n",
    "    text = \"This movie is absolutely wonderful and amazing!\"\n",
    "    attacked_text = AttackedText(text)\n",
    "    \n",
    "    print(f\"Original: {text}\")\n",
    "    print(\"\\nRunning TextFooler attack...\")\n",
    "    \n",
    "    # Note: This may take a minute\n",
    "    # result = attack.attack(attacked_text, 1)  # 1 is the ground truth label\n",
    "    # print(f\"\\nAdversarial: {result.perturbed_text()}\")\n",
    "else:\n",
    "    print(\"Skipping TextAttack demo - not installed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Attack Evaluation\n",
    "\n",
    "### Metrics for Text Attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack Evaluation Results:\n",
      "==================================================\n",
      "success: False\n",
      "original_label: POSITIVE\n",
      "adversarial_label: POSITIVE\n",
      "original_confidence: 99.99%\n",
      "adversarial_confidence: 99.99%\n",
      "semantic_similarity: 91.39%\n",
      "edit_ratio: 70.10%\n",
      "word_overlap: 71.43%\n"
     ]
    }
   ],
   "source": [
    "def evaluate_text_attack(original, adversarial, model_pipeline, sim_model):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation of text attack\n",
    "    \"\"\"\n",
    "    # 1. Attack success\n",
    "    orig_result = model_pipeline(original)[0]\n",
    "    adv_result = model_pipeline(adversarial)[0]\n",
    "    success = orig_result['label'] != adv_result['label']\n",
    "    \n",
    "    # 2. Semantic similarity\n",
    "    orig_emb = sim_model.encode(original, convert_to_tensor=True)\n",
    "    adv_emb = sim_model.encode(adversarial, convert_to_tensor=True)\n",
    "    similarity = util.cos_sim(orig_emb, adv_emb).item()\n",
    "    \n",
    "    # 3. Edit distance\n",
    "    from difflib import SequenceMatcher\n",
    "    edit_ratio = SequenceMatcher(None, original, adversarial).ratio()\n",
    "    \n",
    "    # 4. Word overlap\n",
    "    orig_words = set(original.lower().split())\n",
    "    adv_words = set(adversarial.lower().split())\n",
    "    word_overlap = len(orig_words & adv_words) / len(orig_words) if orig_words else 0\n",
    "    \n",
    "    results = {\n",
    "        'success': success,\n",
    "        'original_label': orig_result['label'],\n",
    "        'adversarial_label': adv_result['label'],\n",
    "        'original_confidence': orig_result['score'],\n",
    "        'adversarial_confidence': adv_result['score'],\n",
    "        'semantic_similarity': similarity,\n",
    "        'edit_ratio': edit_ratio,\n",
    "        'word_overlap': word_overlap\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test evaluation\n",
    "original = \"This movie is absolutely wonderful and amazing!\"\n",
    "adversarial = synonym_attack(original, 0.8)\n",
    "\n",
    "results = evaluate_text_attack(original, adversarial, sentiment_pipeline, sim_model)\n",
    "\n",
    "print(\"Attack Evaluation Results:\")\n",
    "print(\"=\" * 50)\n",
    "for key, value in results.items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"{key}: {value:.2%}\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Implement Keyboard Typo Attack\n",
    "\n",
    "Create an attack that simulates keyboard typos (adjacent key substitutions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keyboard layout (QWERTY)\n",
    "KEYBOARD_ADJACENT = {\n",
    "    'q': ['w', 'a'],\n",
    "    'w': ['q', 'e', 's'],\n",
    "    'e': ['w', 'r', 'd'],\n",
    "    # ... add more\n",
    "}\n",
    "\n",
    "def keyboard_typo_attack(text, typo_rate=0.1):\n",
    "    \"\"\"\n",
    "    TODO: Implement keyboard typo attack\n",
    "    \n",
    "    Hint: Replace characters with adjacent keys on keyboard\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "# Test your implementation\n",
    "# original = \"This movie is wonderful\"\n",
    "# adversarial = keyboard_typo_attack(original)\n",
    "# print(f\"Original: {original}\")\n",
    "# print(f\"With typos: {adversarial}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Optimize Attack for Semantic Similarity\n",
    "\n",
    "Create an attack that maximizes semantic similarity while achieving misclassification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimized_synonym_attack(text, model_pipeline, sim_model, min_similarity=0.9):\n",
    "    \"\"\"\n",
    "    TODO: Implement attack that maintains high semantic similarity\n",
    "    \n",
    "    Hint: Try different synonym combinations and keep the one with highest similarity\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Text attacks are harder** than image attacks due to discrete space\n",
    "2. **Character-level attacks** can evade simple filters\n",
    "3. **Word-level attacks** preserve semantics better\n",
    "4. **Semantic similarity** is crucial for imperceptibility\n",
    "5. **TextAttack** provides production-ready implementations\n",
    "\n",
    "### Attack Comparison\n",
    "\n",
    "| Attack Type | Imperceptibility | Success Rate | Semantic Preservation |\n",
    "|-------------|------------------|--------------|----------------------|\n",
    "| Homoglyphs | High | Medium | High |\n",
    "| Char Insert/Delete | Low | Low | Low |\n",
    "| Synonyms | High | High | High |\n",
    "| TextFooler | High | Very High | High |\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "1. Complete exercises\n",
    "2. Try attacks on different models\n",
    "3. Experiment with TextAttack recipes\n",
    "4. Move to Lab 4: Transfer Attacks"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
