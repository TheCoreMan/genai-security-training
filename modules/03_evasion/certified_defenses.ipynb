{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Certified Defenses Against Adversarial Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Using Apple Silicon GPU (MPS)\n",
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "# Setup and Imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Detect device (supports CUDA, Apple Silicon MPS, and CPU)\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    print(f\"✓ Using CUDA GPU: {torch.cuda.get_device_name(0)}\")\n",
    "elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():\n",
    "    device = torch.device('mps')\n",
    "    print(\"✓ Using Apple Silicon GPU (MPS)\")\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"ℹ Using CPU\")\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ scipy already installed\n",
      "✓ All packages ready!\n"
     ]
    }
   ],
   "source": [
    "# Install required packages for this notebook\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', package])\n",
    "\n",
    "# Check scipy (should be in requirements.txt)\n",
    "try:\n",
    "    import scipy\n",
    "    from scipy.stats import norm\n",
    "    print(\"✓ scipy already installed\")\n",
    "except ImportError:\n",
    "    print(\"Installing scipy...\")\n",
    "    install_package('scipy')\n",
    "    print(\"✓ scipy installed\")\n",
    "\n",
    "print(\"✓ All packages ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "\n",
    "While empirical defenses can be bypassed by adaptive attacks, certified defenses provide provable guarantees of robustness. This document explores techniques that mathematically guarantee model behavior within specified bounds, offering the strongest form of adversarial robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "\n",
    "\n",
    "- Understand certified vs empirical robustness\n",
    "- Implement randomized smoothing\n",
    "- Apply interval bound propagation\n",
    "- Use Lipschitz constraints\n",
    "- Evaluate certified accuracy\n",
    "- Design certifiably robust models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why \"Certified\"?\n",
    "\n",
    "**Certified** = Mathematical proof/certificate that guarantees robustness\n",
    "\n",
    "Think of it like a certificate of authenticity:\n",
    "- A certificate proves something is genuine\n",
    "- A **certified defense** proves the model is robust within specific bounds\n",
    "\n",
    "### Real-World Analogy\n",
    "\n",
    "**Empirical Defense**: \"I tested this bridge with 100 trucks and it didn't collapse\"\n",
    "- ❌ What about the 101st truck?\n",
    "- ❌ What if someone designs a heavier truck?\n",
    "\n",
    "**Certified Defense**: \"I have engineering calculations proving this bridge can hold up to 50 tons\"\n",
    "- ✓ This is a **certificate** of safety\n",
    "- ✓ Doesn't matter what truck you design - if it's under 50 tons, the bridge is guaranteed safe\n",
    "\n",
    "### In Machine Learning\n",
    "\n",
    "**Empirical**: \"Robust against attacks we tested\"\n",
    "- Tested against FGSM, PGD, C&W\n",
    "- No guarantee against unknown attacks\n",
    "\n",
    "**Certified**: \"Provably robust against ALL attacks in threat model\"\n",
    "- Mathematical guarantee: \"ANY perturbation with L2 norm < 0.5 cannot fool the model\"\n",
    "- Holds for all possible attacks, not just tested ones\n",
    "\n",
    "---\n",
    "\n",
    "## Why Certification Matters\n",
    "\n",
    "\n",
    "### Empirical vs Certified Robustness\n",
    "\n",
    "**Empirical Defense**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empirical Defense Approach:\n",
      "1. Train model with adversarial examples\n",
      "2. Test against known attacks (FGSM, PGD)\n",
      "3. Report accuracy on tested attacks\n",
      "\n",
      "⚠️  Problem: Might fail against unknown/adaptive attacks\n",
      "   Only robust against attacks we tested!\n"
     ]
    }
   ],
   "source": [
    "# Example: Empirical Defense (Conceptual)\n",
    "# This shows the LIMITATION of empirical defenses\n",
    "\n",
    "print(\"Empirical Defense Approach:\")\n",
    "print(\"1. Train model with adversarial examples\")\n",
    "print(\"2. Test against known attacks (FGSM, PGD)\")\n",
    "print(\"3. Report accuracy on tested attacks\")\n",
    "print(\"\\n⚠️  Problem: Might fail against unknown/adaptive attacks\")\n",
    "print(\"   Only robust against attacks we tested!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Certified Defense**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certified Defense Approach:\n",
      "1. Train model with certification objective\n",
      "2. Compute provable robustness radius\n",
      "3. Guarantee: No attack within epsilon can fool model\n",
      "\n",
      "✓ Advantage: Holds for ALL possible attacks\n",
      "  Not just the ones we tested!\n",
      "\n",
      "Example guarantee:\n",
      "  'For input x, any perturbation with L2 norm < 0.5'\n",
      "  'will not change the model prediction'\n"
     ]
    }
   ],
   "source": [
    "# Example: Certified Defense (Conceptual)\n",
    "# This shows the ADVANTAGE of certified defenses\n",
    "\n",
    "print(\"Certified Defense Approach:\")\n",
    "print(\"1. Train model with certification objective\")\n",
    "print(\"2. Compute provable robustness radius\")\n",
    "print(\"3. Guarantee: No attack within epsilon can fool model\")\n",
    "print(\"\\n✓ Advantage: Holds for ALL possible attacks\")\n",
    "print(\"  Not just the ones we tested!\")\n",
    "print(\"\\nExample guarantee:\")\n",
    "epsilon = 0.5\n",
    "print(f\"  'For input x, any perturbation with L2 norm < {epsilon}'\")\n",
    "print(f\"  'will not change the model prediction'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Security Guarantees\n",
    "\n",
    "```\n",
    "Empirical: \"Robust against attacks we tested\"\n",
    "Certified: \"Provably robust against ALL attacks in threat model\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomized Smoothing\n",
    "\n",
    "\n",
    "### Core Concept\n",
    "\n",
    "**Idea**: Average predictions over random noise to create smooth, robust classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This is an example function showing the structure.\n",
    "# To run it, you would need to load MNIST data and a trained model.\n",
    "# See Lab 5 for a complete working implementation.\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "class RandomizedSmoothing:\n",
    "    \"\"\"\n",
    "    Certified defense via randomized smoothing\n",
    "    \n",
    "    Reference: \"Certified Adversarial Robustness via Randomized Smoothing\"\n",
    "               (Cohen et al., 2019)\n",
    "    \"\"\"\n",
    "    def __init__(self, base_classifier, sigma, num_samples=100):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            base_classifier: Base model to smooth\n",
    "            sigma: Noise standard deviation\n",
    "            num_samples: Number of noise samples for prediction\n",
    "        \"\"\"\n",
    "        self.base_classifier = base_classifier\n",
    "        self.sigma = sigma\n",
    "        self.num_samples = num_samples\n",
    "    \n",
    "    def predict(self, x):\n",
    "        \"\"\"\n",
    "        Smoothed prediction by averaging over noise\n",
    "        \"\"\"\n",
    "        counts = np.zeros(self.num_classes)\n",
    "        \n",
    "        for _ in range(self.num_samples):\n",
    "            # Add Gaussian noise\n",
    "            noise = torch.randn_like(x) * self.sigma\n",
    "            x_noisy = x + noise\n",
    "            \n",
    "            # Get prediction\n",
    "            pred = self.base_classifier(x_noisy)\n",
    "            predicted_class = pred.argmax()\n",
    "            counts[predicted_class] += 1\n",
    "        \n",
    "        # Return most common prediction\n",
    "        return counts.argmax()\n",
    "    \n",
    "    def certify(self, x, n_samples=10000, alpha=0.001):\n",
    "        \"\"\"\n",
    "        Compute certified radius for input x\n",
    "        \n",
    "        Returns:\n",
    "            predicted_class: Predicted class\n",
    "            radius: Certified L2 radius (or None if certification fails)\n",
    "        \"\"\"\n",
    "        # Count predictions with many samples\n",
    "        counts = np.zeros(self.num_classes)\n",
    "        \n",
    "        for _ in range(n_samples):\n",
    "            noise = torch.randn_like(x) * self.sigma\n",
    "            x_noisy = x + noise\n",
    "            pred = self.base_classifier(x_noisy)\n",
    "            counts[pred.argmax()] += 1\n",
    "        \n",
    "        # Get top two classes\n",
    "        top_counts = np.sort(counts)[-2:]\n",
    "        top_class = counts.argmax()\n",
    "        \n",
    "        # Compute confidence bounds using Clopper-Pearson\n",
    "        p_lower = self._lower_confidence_bound(\n",
    "            counts[top_class], n_samples, alpha\n",
    "        )\n",
    "        \n",
    "        # Compute certified radius\n",
    "        if p_lower > 0.5:\n",
    "            radius = self.sigma * norm.ppf(p_lower)\n",
    "            return top_class, radius\n",
    "        else:\n",
    "            return top_class, None  # Certification failed\n",
    "    \n",
    "    def _lower_confidence_bound(self, count, n, alpha):\n",
    "        \"\"\"\n",
    "        Compute lower confidence bound on probability\n",
    "        \"\"\"\n",
    "        from statsmodels.stats.proportion import proportion_confint\n",
    "        lower, _ = proportion_confint(count, n, alpha=2*alpha, method='beta')\n",
    "        return lower\n",
    "\n",
    "# Example usage\n",
    "def example_randomized_smoothing():\n",
    "    \"\"\"\n",
    "    Example: Certify MNIST classifier\n",
    "    \"\"\"\n",
    "    # Load base classifier\n",
    "    base_model = load_mnist_classifier()\n",
    "    \n",
    "    # Create smoothed classifier\n",
    "    smoothed = RandomizedSmoothing(\n",
    "        base_classifier=base_model,\n",
    "        sigma=0.25,  # Noise level\n",
    "        num_samples=100\n",
    "    )\n",
    "    \n",
    "    # Test on example\n",
    "    x, y = load_test_example()\n",
    "    \n",
    "    # Get certified radius\n",
    "    pred_class, radius = smoothed.certify(x, n_samples=10000)\n",
    "    \n",
    "    if radius is not None:\n",
    "        print(f\"Prediction: {pred_class}\")\n",
    "        print(f\"Certified L2 radius: {radius:.4f}\")\n",
    "        print(f\"Guarantee: No L2 perturbation < {radius} can change prediction\")\n",
    "    else:\n",
    "        print(\"Certification failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working Example: Simple Randomized Smoothing\n",
    "\n",
    "Let's implement a simplified version that demonstrates the concept:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating simple example...\n",
      "\n",
      "Regular prediction: Class 0\n",
      "Smoothed prediction: Class 0\n",
      "\n",
      "✓ Smoothed prediction is more robust to small perturbations!\n",
      "\n",
      "Testing robustness to perturbations:\n",
      "  Regular: 0 → 0 (changed: False)\n",
      "  Smoothed: 0 → 0 (changed: False)\n"
     ]
    }
   ],
   "source": [
    "# Working Example: Simple Randomized Smoothing\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from scipy.stats import norm\n",
    "\n",
    "print(\"Creating simple example...\")\n",
    "\n",
    "# Create a simple classifier\n",
    "class SimpleClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(10, 3)  # 10 features, 3 classes\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "model = SimpleClassifier().to(device)\n",
    "\n",
    "# Simple randomized smoothing\n",
    "def smooth_predict(model, x, sigma=0.1, num_samples=100):\n",
    "    \"\"\"Predict by averaging over noisy samples\"\"\"\n",
    "    counts = torch.zeros(3)  # 3 classes\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        # Add Gaussian noise\n",
    "        noise = torch.randn_like(x) * sigma\n",
    "        x_noisy = x + noise\n",
    "        \n",
    "        # Get prediction\n",
    "        with torch.no_grad():\n",
    "            pred = model(x_noisy).argmax()\n",
    "            counts[pred] += 1\n",
    "    \n",
    "    return counts.argmax().item()\n",
    "\n",
    "# Test it\n",
    "x = torch.randn(1, 10).to(device)\n",
    "\n",
    "# Regular prediction\n",
    "with torch.no_grad():\n",
    "    regular_pred = model(x).argmax().item()\n",
    "\n",
    "# Smoothed prediction\n",
    "smooth_pred = smooth_predict(model, x, sigma=0.1, num_samples=100)\n",
    "\n",
    "print(f\"\\nRegular prediction: Class {regular_pred}\")\n",
    "print(f\"Smoothed prediction: Class {smooth_pred}\")\n",
    "print(f\"\\n✓ Smoothed prediction is more robust to small perturbations!\")\n",
    "\n",
    "# Demonstrate robustness\n",
    "print(f\"\\nTesting robustness to perturbations:\")\n",
    "epsilon = 0.05\n",
    "perturbation = torch.randn_like(x) * epsilon\n",
    "x_perturbed = x + perturbation\n",
    "\n",
    "with torch.no_grad():\n",
    "    regular_pred_perturbed = model(x_perturbed).argmax().item()\n",
    "smooth_pred_perturbed = smooth_predict(model, x_perturbed, sigma=0.1, num_samples=100)\n",
    "\n",
    "print(f\"  Regular: {regular_pred} → {regular_pred_perturbed} (changed: {regular_pred != regular_pred_perturbed})\")\n",
    "print(f\"  Smoothed: {smooth_pred} → {smooth_pred_perturbed} (changed: {smooth_pred != smooth_pred_perturbed})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training for Randomized Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_for_smoothing(model, train_loader, sigma, epochs=100):\n",
    "    \"\"\"\n",
    "    Train base classifier for randomized smoothing\n",
    "    \n",
    "    Key: Train with Gaussian noise augmentation\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for x, y in train_loader:\n",
    "            # Add Gaussian noise during training\n",
    "            noise = torch.randn_like(x) * sigma\n",
    "            x_noisy = x + noise\n",
    "            \n",
    "            # Standard training\n",
    "            optimizer.zero_grad()\n",
    "            output = model(x_noisy)\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Certified Accuracy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_certified_accuracy(smoothed_model, test_loader, epsilon):\n",
    "    \"\"\"\n",
    "    Evaluate certified accuracy at radius epsilon\n",
    "    \"\"\"\n",
    "    certified_correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for x, y in test_loader:\n",
    "        pred_class, radius = smoothed_model.certify(x)\n",
    "        \n",
    "        # Count as certified correct if:\n",
    "        # 1. Prediction is correct\n",
    "        # 2. Certified radius >= epsilon\n",
    "        if pred_class == y and radius is not None and radius >= epsilon:\n",
    "            certified_correct += 1\n",
    "        \n",
    "        total += 1\n",
    "    \n",
    "    certified_accuracy = certified_correct / total\n",
    "    return certified_accuracy\n",
    "\n",
    "# Example results\n",
    "# Epsilon | Certified Accuracy\n",
    "# 0.0     | 95%  (clean accuracy)\n",
    "# 0.25    | 82%\n",
    "# 0.5     | 68%\n",
    "# 1.0     | 45%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interval Bound Propagation (IBP)\n",
    "\n",
    "\n",
    "### Core Concept\n",
    "\n",
    "**Idea**: Propagate input bounds through network to certify output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntervalBoundPropagation:\n",
    "    \"\"\"\n",
    "    Certified defense via interval bound propagation\n",
    "    \n",
    "    Reference: \"Towards Fast Computation of Certified Robustness for \n",
    "                ReLU Networks\" (Weng et al., 2018)\n",
    "    \"\"\"\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    def compute_bounds(self, x, epsilon):\n",
    "        \"\"\"\n",
    "        Compute output bounds for input in [x-epsilon, x+epsilon]\n",
    "        \"\"\"\n",
    "        # Initialize input bounds\n",
    "        lower = x - epsilon\n",
    "        upper = x + epsilon\n",
    "        \n",
    "        # Propagate through each layer\n",
    "        for layer in self.model.layers:\n",
    "            lower, upper = self.propagate_layer(layer, lower, upper)\n",
    "        \n",
    "        return lower, upper\n",
    "    \n",
    "    def propagate_layer(self, layer, lower, upper):\n",
    "        \"\"\"\n",
    "        Propagate bounds through a single layer\n",
    "        \"\"\"\n",
    "        if isinstance(layer, torch.nn.Linear):\n",
    "            return self.propagate_linear(layer, lower, upper)\n",
    "        elif isinstance(layer, torch.nn.ReLU):\n",
    "            return self.propagate_relu(lower, upper)\n",
    "        elif isinstance(layer, torch.nn.Conv2d):\n",
    "            return self.propagate_conv(layer, lower, upper)\n",
    "        else:\n",
    "            raise NotImplementedError(f\"Layer {type(layer)} not supported\")\n",
    "    \n",
    "    def propagate_linear(self, layer, lower, upper):\n",
    "        \"\"\"\n",
    "        Propagate through linear layer: y = Wx + b\n",
    "        \"\"\"\n",
    "        W = layer.weight\n",
    "        b = layer.bias\n",
    "        \n",
    "        # Compute bounds\n",
    "        # For positive weights: lower bound uses lower input\n",
    "        # For negative weights: lower bound uses upper input\n",
    "        W_pos = torch.clamp(W, min=0)\n",
    "        W_neg = torch.clamp(W, max=0)\n",
    "        \n",
    "        new_lower = W_pos @ lower + W_neg @ upper + b\n",
    "        new_upper = W_pos @ upper + W_neg @ lower + b\n",
    "        \n",
    "        return new_lower, new_upper\n",
    "    \n",
    "    def propagate_relu(self, lower, upper):\n",
    "        \"\"\"\n",
    "        Propagate through ReLU: y = max(0, x)\n",
    "        \"\"\"\n",
    "        # ReLU is monotonic\n",
    "        new_lower = torch.clamp(lower, min=0)\n",
    "        new_upper = torch.clamp(upper, min=0)\n",
    "        \n",
    "        return new_lower, new_upper\n",
    "    \n",
    "    def certify(self, x, y_true, epsilon):\n",
    "        \"\"\"\n",
    "        Certify that prediction is robust within epsilon\n",
    "        \"\"\"\n",
    "        # Compute output bounds\n",
    "        lower, upper = self.compute_bounds(x, epsilon)\n",
    "        \n",
    "        # Check if true class has highest lower bound\n",
    "        # This guarantees it's the prediction for all inputs in ball\n",
    "        if lower[y_true] > upper.max(dim=0)[0]:\n",
    "            return True, epsilon\n",
    "        else:\n",
    "            return False, None\n",
    "\n",
    "# Example usage\n",
    "def example_ibp():\n",
    "    \"\"\"\n",
    "    Example: Certify with IBP\n",
    "    \"\"\"\n",
    "    model = load_simple_network()\n",
    "    ibp = IntervalBoundPropagation(model)\n",
    "    \n",
    "    x, y = load_test_example()\n",
    "    epsilon = 0.1\n",
    "    \n",
    "    is_certified, radius = ibp.certify(x, y, epsilon)\n",
    "    \n",
    "    if is_certified:\n",
    "        print(f\"Certified robust within L∞ radius {radius}\")\n",
    "    else:\n",
    "        print(\"Not certified at this radius\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with IBP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_ibp(model, train_loader, epsilon, epochs=100):\n",
    "    \"\"\"\n",
    "    Train model to be certifiably robust using IBP\n",
    "    \n",
    "    Reference: \"Certified Adversarial Robustness via Randomized Smoothing\"\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "    ibp = IntervalBoundPropagation(model)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for x, y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Compute bounds on output\n",
    "            lower, upper = ibp.compute_bounds(x, epsilon)\n",
    "            \n",
    "            # Loss: Maximize lower bound of correct class\n",
    "            #       Minimize upper bound of incorrect classes\n",
    "            correct_class_lower = lower[range(len(y)), y]\n",
    "            \n",
    "            # Get max upper bound of incorrect classes\n",
    "            mask = torch.ones_like(lower)\n",
    "            mask[range(len(y)), y] = 0\n",
    "            incorrect_upper = (upper * mask).max(dim=1)[0]\n",
    "            \n",
    "            # Robust loss: margin between correct and incorrect\n",
    "            loss = torch.relu(incorrect_upper - correct_class_lower + 1.0).mean()\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lipschitz Constraints\n",
    "\n",
    "\n",
    "### Core Concept\n",
    "\n",
    "**Idea**: Bound how much output can change relative to input change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lipschitz_constant(model):\n",
    "    \"\"\"\n",
    "    Compute Lipschitz constant of model\n",
    "    \n",
    "    For neural network: L = product of layer Lipschitz constants\n",
    "    For linear layer: L = largest singular value of weight matrix\n",
    "    \"\"\"\n",
    "    lipschitz = 1.0\n",
    "    \n",
    "    for layer in model.layers:\n",
    "        if isinstance(layer, torch.nn.Linear):\n",
    "            # Lipschitz constant is largest singular value\n",
    "            singular_values = torch.svd(layer.weight)[1]\n",
    "            layer_lipschitz = singular_values.max()\n",
    "            lipschitz *= layer_lipschitz\n",
    "        elif isinstance(layer, torch.nn.ReLU):\n",
    "            # ReLU has Lipschitz constant 1\n",
    "            lipschitz *= 1.0\n",
    "    \n",
    "    return lipschitz\n",
    "\n",
    "def certified_radius_from_lipschitz(model, x, margin):\n",
    "    \"\"\"\n",
    "    Compute certified radius using Lipschitz constant\n",
    "    \n",
    "    If model has Lipschitz constant L and margin m,\n",
    "    then certified radius is m/L\n",
    "    \"\"\"\n",
    "    L = lipschitz_constant(model)\n",
    "    \n",
    "    # Compute margin: difference between top two class scores\n",
    "    output = model(x)\n",
    "    top_two = torch.topk(output, 2)[0]\n",
    "    margin = top_two[0] - top_two[1]\n",
    "    \n",
    "    # Certified radius\n",
    "    radius = margin / L\n",
    "    \n",
    "    return radius"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lipschitz-Constrained Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LipschitzLinear(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Linear layer with Lipschitz constraint\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, lipschitz_bound=1.0):\n",
    "        super().__init__()\n",
    "        self.weight = torch.nn.Parameter(\n",
    "            torch.randn(out_features, in_features)\n",
    "        )\n",
    "        self.bias = torch.nn.Parameter(torch.zeros(out_features))\n",
    "        self.lipschitz_bound = lipschitz_bound\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Normalize weight to satisfy Lipschitz constraint\n",
    "        W = self.weight\n",
    "        singular_values = torch.svd(W)[1]\n",
    "        max_sv = singular_values.max()\n",
    "        \n",
    "        if max_sv > self.lipschitz_bound:\n",
    "            W = W * (self.lipschitz_bound / max_sv)\n",
    "        \n",
    "        return torch.nn.functional.linear(x, W, self.bias)\n",
    "\n",
    "def build_lipschitz_network(input_dim, hidden_dims, output_dim, \n",
    "                             lipschitz_bound=1.0):\n",
    "    \"\"\"\n",
    "    Build network with Lipschitz constraint\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "    \n",
    "    # Input layer\n",
    "    layers.append(LipschitzLinear(input_dim, hidden_dims[0], lipschitz_bound))\n",
    "    layers.append(torch.nn.ReLU())\n",
    "    \n",
    "    # Hidden layers\n",
    "    for i in range(len(hidden_dims) - 1):\n",
    "        layers.append(LipschitzLinear(\n",
    "            hidden_dims[i], hidden_dims[i+1], lipschitz_bound\n",
    "        ))\n",
    "        layers.append(torch.nn.ReLU())\n",
    "    \n",
    "    # Output layer\n",
    "    layers.append(LipschitzLinear(hidden_dims[-1], output_dim, lipschitz_bound))\n",
    "    \n",
    "    return torch.nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spectral Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpectralNorm(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Spectral normalization for Lipschitz constraint\n",
    "    \n",
    "    Reference: \"Spectral Normalization for Generative Adversarial Networks\"\n",
    "               (Miyato et al., 2018)\n",
    "    \"\"\"\n",
    "    def __init__(self, module, name='weight', power_iterations=1):\n",
    "        super().__init__()\n",
    "        self.module = module\n",
    "        self.name = name\n",
    "        self.power_iterations = power_iterations\n",
    "        \n",
    "        # Initialize u and v vectors for power iteration\n",
    "        w = getattr(module, name)\n",
    "        height = w.shape[0]\n",
    "        width = w.view(height, -1).shape[1]\n",
    "        \n",
    "        self.register_buffer('u', torch.randn(height))\n",
    "        self.register_buffer('v', torch.randn(width))\n",
    "    \n",
    "    def forward(self, *args):\n",
    "        # Compute spectral norm via power iteration\n",
    "        w = getattr(self.module, self.name)\n",
    "        height = w.shape[0]\n",
    "        w_mat = w.view(height, -1)\n",
    "        \n",
    "        # Power iteration\n",
    "        u = self.u\n",
    "        v = self.v\n",
    "        for _ in range(self.power_iterations):\n",
    "            v = torch.nn.functional.normalize(w_mat.t() @ u, dim=0)\n",
    "            u = torch.nn.functional.normalize(w_mat @ v, dim=0)\n",
    "        \n",
    "        # Compute spectral norm\n",
    "        sigma = u @ w_mat @ v\n",
    "        \n",
    "        # Normalize weight\n",
    "        w_normalized = w / sigma\n",
    "        setattr(self.module, self.name, w_normalized)\n",
    "        \n",
    "        # Update u and v\n",
    "        self.u = u\n",
    "        self.v = v\n",
    "        \n",
    "        return self.module(*args)\n",
    "\n",
    "# Apply spectral normalization\n",
    "def apply_spectral_norm(model):\n",
    "    \"\"\"\n",
    "    Apply spectral normalization to all linear layers\n",
    "    \"\"\"\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.Linear):\n",
    "            spectral_norm = SpectralNorm(module)\n",
    "            setattr(model, name, spectral_norm)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convex Relaxations\n",
    "\n",
    "\n",
    "### Linear Relaxation of ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_relaxation_relu(lower, upper):\n",
    "    \"\"\"\n",
    "    Compute linear relaxation of ReLU\n",
    "    \n",
    "    For x in [lower, upper], ReLU(x) is bounded by:\n",
    "    - Lower bound: max(0, lower) if lower >= 0\n",
    "                   0 if upper <= 0\n",
    "                   linear interpolation otherwise\n",
    "    - Upper bound: linear interpolation from (lower, ReLU(lower)) \n",
    "                   to (upper, ReLU(upper))\n",
    "    \"\"\"\n",
    "    if lower >= 0:\n",
    "        # ReLU is active\n",
    "        return lower, upper\n",
    "    elif upper <= 0:\n",
    "        # ReLU is inactive\n",
    "        return 0, 0\n",
    "    else:\n",
    "        # ReLU is ambiguous - use linear relaxation\n",
    "        # Lower bound: 0 (ReLU could be inactive)\n",
    "        # Upper bound: line from (lower, 0) to (upper, upper)\n",
    "        slope = upper / (upper - lower)\n",
    "        intercept = -slope * lower\n",
    "        \n",
    "        return 0, slope * upper + intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CROWN (Certified ROBustness via Optimization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CROWN:\n",
    "    \"\"\"\n",
    "    CROWN: Efficient certified bounds via linear relaxation\n",
    "    \n",
    "    Reference: \"Efficient Neural Network Robustness Certification with \n",
    "                General Activation Functions\" (Zhang et al., 2018)\n",
    "    \"\"\"\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    def compute_bounds(self, x, epsilon):\n",
    "        \"\"\"\n",
    "        Compute certified bounds using CROWN\n",
    "        \"\"\"\n",
    "        # Initialize bounds\n",
    "        lower = x - epsilon\n",
    "        upper = x + epsilon\n",
    "        \n",
    "        # Backward bound propagation\n",
    "        for layer in reversed(self.model.layers):\n",
    "            if isinstance(layer, torch.nn.Linear):\n",
    "                lower, upper = self.backward_linear(layer, lower, upper)\n",
    "            elif isinstance(layer, torch.nn.ReLU):\n",
    "                lower, upper = self.backward_relu(lower, upper)\n",
    "        \n",
    "        return lower, upper\n",
    "    \n",
    "    def backward_linear(self, layer, lower, upper):\n",
    "        \"\"\"\n",
    "        Backward propagation through linear layer\n",
    "        \"\"\"\n",
    "        W = layer.weight\n",
    "        b = layer.bias\n",
    "        \n",
    "        # Compute bounds (similar to IBP but with optimization)\n",
    "        W_pos = torch.clamp(W, min=0)\n",
    "        W_neg = torch.clamp(W, max=0)\n",
    "        \n",
    "        new_lower = W_pos @ lower + W_neg @ upper + b\n",
    "        new_upper = W_pos @ upper + W_neg @ lower + b\n",
    "        \n",
    "        return new_lower, new_upper\n",
    "    \n",
    "    def backward_relu(self, lower, upper):\n",
    "        \"\"\"\n",
    "        Backward propagation through ReLU with linear relaxation\n",
    "        \"\"\"\n",
    "        # Use linear relaxation for ambiguous neurons\n",
    "        return linear_relaxation_relu(lower, upper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of Methods\n",
    "\n",
    "\n",
    "### Certified Accuracy vs Epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_certified_methods(test_loader):\n",
    "    \"\"\"\n",
    "    Compare different certification methods\n",
    "    \"\"\"\n",
    "    methods = {\n",
    "        'Randomized Smoothing': RandomizedSmoothing(model, sigma=0.25),\n",
    "        'IBP': IntervalBoundPropagation(model),\n",
    "        'Lipschitz': LipschitzConstrained(model),\n",
    "        'CROWN': CROWN(model)\n",
    "    }\n",
    "    \n",
    "    epsilons = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "    results = {method: [] for method in methods}\n",
    "    \n",
    "    for epsilon in epsilons:\n",
    "        for method_name, method in methods.items():\n",
    "            accuracy = evaluate_certified_accuracy(method, test_loader, epsilon)\n",
    "            results[method_name].append(accuracy)\n",
    "    \n",
    "    # Plot results\n",
    "    import matplotlib.pyplot as plt\n",
    "    for method_name, accuracies in results.items():\n",
    "        plt.plot(epsilons, accuracies, label=method_name)\n",
    "    plt.xlabel('Epsilon')\n",
    "    plt.ylabel('Certified Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trade-offs\n",
    "\n",
    "| Method | Tightness | Scalability | Training Cost |\n",
    "|--------|-----------|-------------|---------------|\n",
    "| Randomized Smoothing | Moderate | High | Low |\n",
    "| IBP | Loose | High | Moderate |\n",
    "| Lipschitz | Loose | High | Low |\n",
    "| CROWN | Tight | Moderate | Moderate |\n",
    "| Exact Verification | Tightest | Low | High |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practical Considerations\n",
    "\n",
    "\n",
    "### When to Use Certified Defenses\n",
    "\n",
    "**Use When**:\n",
    "- High-security applications\n",
    "- Need provable guarantees\n",
    "- Regulatory requirements\n",
    "- Critical infrastructure\n",
    "- Safety-critical systems\n",
    "\n",
    "**Consider Alternatives When**:\n",
    "- Accuracy is paramount\n",
    "- Computational resources limited\n",
    "- Threat model unclear\n",
    "- Rapid iteration needed\n",
    "\n",
    "### Limitations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid decimal literal (476864537.py, line 7)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mcertification_time = 100x * inference_time\u001b[39m\n                           ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid decimal literal\n"
     ]
    }
   ],
   "source": [
    "# Limitation 1: Accuracy-Robustness Trade-off\n",
    "clean_accuracy = 95%\n",
    "certified_accuracy_at_epsilon_0.5 = 65%\n",
    "# Must sacrifice some accuracy for certification\n",
    "\n",
    "# Limitation 2: Computational Cost\n",
    "certification_time = 100x * inference_time\n",
    "# Certification is expensive\n",
    "\n",
    "# Limitation 3: Limited Threat Models\n",
    "# Most methods certify L2 or L∞ perturbations\n",
    "# Don't cover all possible attacks\n",
    "\n",
    "# Limitation 4: Scalability\n",
    "# Harder to certify large models (e.g., GPT-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def certified_defense_best_practices():\n",
    "    \"\"\"\n",
    "    Best practices for certified defenses\n",
    "    \"\"\"\n",
    "    practices = [\n",
    "        \"1. Choose appropriate threat model (L2, L∞, etc.)\",\n",
    "        \"2. Balance accuracy and robustness\",\n",
    "        \"3. Use appropriate certification method for scale\",\n",
    "        \"4. Train specifically for certification\",\n",
    "        \"5. Validate on diverse test set\",\n",
    "        \"6. Monitor computational costs\",\n",
    "        \"7. Combine with empirical defenses\",\n",
    "        \"8. Regular security audits\"\n",
    "    ]\n",
    "    return practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Topics\n",
    "\n",
    "\n",
    "### Certified Defense for Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def certify_transformer(model, x, epsilon):\n",
    "    \"\"\"\n",
    "    Certify transformer model\n",
    "    \n",
    "    Challenge: Attention mechanism is complex\n",
    "    Solution: Use IBP with careful bound propagation\n",
    "    \"\"\"\n",
    "    # Propagate bounds through embedding\n",
    "    lower, upper = propagate_embedding(x, epsilon)\n",
    "    \n",
    "    # Propagate through attention layers\n",
    "    for layer in model.transformer_layers:\n",
    "        lower, upper = propagate_attention(layer, lower, upper)\n",
    "    \n",
    "    # Propagate through output\n",
    "    lower, upper = propagate_output(model.output_layer, lower, upper)\n",
    "    \n",
    "    return lower, upper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Certified Defense Against Semantic Attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def certify_semantic_robustness(model, x, semantic_transformations):\n",
    "    \"\"\"\n",
    "    Certify robustness to semantic transformations\n",
    "    \n",
    "    Example: Paraphrases, synonyms, etc.\n",
    "    \"\"\"\n",
    "    # Enumerate all possible semantic transformations\n",
    "    transformed_inputs = [\n",
    "        transform(x) for transform in semantic_transformations\n",
    "    ]\n",
    "    \n",
    "    # Check if all transformations give same prediction\n",
    "    predictions = [model(x_t) for x_t in transformed_inputs]\n",
    "    \n",
    "    if all(p == predictions[0] for p in predictions):\n",
    "        return True, \"Certified against semantic transformations\"\n",
    "    else:\n",
    "        return False, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Certified and Empirical Defenses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_defense(model, x):\n",
    "    \"\"\"\n",
    "    Combine certified and empirical defenses\n",
    "    \"\"\"\n",
    "    # Layer 1: Empirical defense (adversarial training)\n",
    "    x_defended = adversarial_training_defense(x)\n",
    "    \n",
    "    # Layer 2: Certified defense (randomized smoothing)\n",
    "    smoothed_model = RandomizedSmoothing(model, sigma=0.25)\n",
    "    pred, radius = smoothed_model.certify(x_defended)\n",
    "    \n",
    "    return pred, radius"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Studies\n",
    "\n",
    "\n",
    "### Case 1: Autonomous Vehicle Perception\n",
    "\n",
    "**Requirement**: Certify stop sign detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must guarantee: No perturbation < epsilon changes detection\n",
    "# Method: Randomized smoothing\n",
    "# Result: Certified radius of 0.3 (L2 norm)\n",
    "# Impact: Provable safety guarantee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 2: Medical Diagnosis\n",
    "\n",
    "**Requirement**: Certify disease classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must guarantee: Robust to measurement noise\n",
    "# Method: IBP with Lipschitz constraints\n",
    "# Result: Certified against ±10% measurement variation\n",
    "# Impact: Regulatory approval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case 3: Financial Fraud Detection\n",
    "\n",
    "**Requirement**: Certify fraud detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Must guarantee: Robust to adversarial manipulation\n",
    "# Method: CROWN certification\n",
    "# Result: Certified against L∞ perturbations of 0.1\n",
    "# Impact: Reduced false negatives from adversarial attacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools and Libraries\n",
    "\n",
    "\n",
    "### Certification Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AutoLiRPA (Automatic Linear Relaxation based Perturbation Analysis)\n",
    "from auto_LiRPA import BoundedModule, BoundedTensor, PerturbationLpNorm\n",
    "\n",
    "model = BoundedModule(model, torch.zeros(input_shape))\n",
    "ptb = PerturbationLpNorm(norm=np.inf, eps=epsilon)\n",
    "bounded_input = BoundedTensor(x, ptb)\n",
    "lower, upper = model.compute_bounds(x=(bounded_input,))\n",
    "\n",
    "# CROWN\n",
    "from crown import CROWN\n",
    "crown = CROWN(model)\n",
    "certified = crown.certify(x, epsilon)\n",
    "\n",
    "# Randomized Smoothing (smoothing library)\n",
    "from smoothing import Smooth\n",
    "smooth = Smooth(model, num_classes, sigma)\n",
    "prediction, radius = smooth.certify(x, n=10000, alpha=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Provable Guarantees**: Certified defenses provide mathematical guarantees\n",
    "2. **Multiple Methods**: Randomized smoothing, IBP, Lipschitz, CROWN\n",
    "3. **Trade-offs**: Accuracy vs robustness, tightness vs scalability\n",
    "4. **Strongest Defense**: Certified defenses are strongest form of robustness\n",
    "5. **Practical Limits**: Computational cost and scalability challenges\n",
    "\n",
    "### When to Use Each Method\n",
    "\n",
    "**Randomized Smoothing**:\n",
    "- Need tight bounds\n",
    "- Can afford inference cost\n",
    "- L2 threat model\n",
    "\n",
    "**IBP**:\n",
    "- Need fast certification\n",
    "- Training time is critical\n",
    "- Can accept looser bounds\n",
    "\n",
    "**Lipschitz Constraints**:\n",
    "- Simple implementation\n",
    "- Moderate robustness sufficient\n",
    "- Want interpretable guarantees\n",
    "\n",
    "**CROWN**:\n",
    "- Need tight bounds\n",
    "- Moderate-sized models\n",
    "- Can afford computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "\n",
    "### Key Papers\n",
    "\n",
    "1. \"Certified Adversarial Robustness via Randomized Smoothing\" (Cohen et al., 2019)\n",
    "2. \"Towards Fast Computation of Certified Robustness for ReLU Networks\" (Weng et al., 2018)\n",
    "3. \"Efficient Neural Network Robustness Certification with General Activation Functions\" (Zhang et al., 2018)\n",
    "4. \"Provable Defenses against Adversarial Examples via the Convex Outer Adversarial Polytope\" (Wong & Kolter, 2018)\n",
    "5. \"Scaling provable adversarial defenses\" (Gowal et al., 2018)\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [AutoLiRPA](https://github.com/Verified-Intelligence/auto_LiRPA)\n",
    "- [CROWN](https://github.com/huanzhang12/CROWN-IBP)\n",
    "- [Randomized Smoothing](https://github.com/locuslab/smoothing)\n",
    "- [Certified Defenses Survey](https://arxiv.org/abs/2009.04131)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "\n",
    "1. Complete [Lab 5: Certified Robustness](labs/lab5_certified_robustness.ipynb)\n",
    "2. Implement randomized smoothing\n",
    "3. Compare certification methods\n",
    "4. Evaluate certified accuracy\n",
    "5. Study latest certification research\n",
    "\n",
    "---\n",
    "\n",
    "**Difficulty**: ⭐⭐⭐⭐⭐ Expert Level\n",
    "**Prerequisites**: Optimization, probability theory, linear algebra\n",
    "**Estimated Time**: 4-5 hours"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
